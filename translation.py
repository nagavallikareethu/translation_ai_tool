# =============================================================
# Unified Translation Pipeline Script
# -------------------------------------------------------------
# Generated by combining pdf_to_json_converter.py, json_translator.py,
# and pdf_creation.py into a single executable file.
# =============================================================
import argparse
import base64
import copy
import html
import io
import json
import math
import os
import re
import tempfile
import time
from datetime import datetime
from io import BytesIO
from pathlib import Path
from typing import Any, Dict, List

import fitz
import pdfplumber
from deep_translator import GoogleTranslator
from playwright.sync_api import sync_playwright
from PyPDF2 import PdfMerger, PdfReader, PdfWriter
from reportlab.lib import colors
from reportlab.lib.utils import ImageReader
from reportlab.pdfbase import pdfmetrics
from reportlab.pdfbase.ttfonts import TTFont
from reportlab.pdfgen import canvas


# ======================================================================
# PDF √¢‚Ä†‚Äô JSON Converter
# ======================================================================
# ============================================================
#  UNIFIED COORDINATE SYSTEM
# ============================================================
class CoordinateConverter:
    """Convert all coordinates to consistent system (Y=0 at bottom)"""
    
    def __init__(self, page_height=842):
        self.page_height = page_height
    
    def pdfplumber_to_standard(self, y):
        """Convert pdfplumber coordinates (Y=0 at top) to standard (Y=0 at bottom)"""
        return self.page_height - y
    
    def pymupdf_to_standard(self, y, height=0):
        """Convert PyMuPDF coordinates to standard"""
        return self.page_height - y - height


# ============================================================
#  ENHANCED TEXT CLEANING - SEPARATE MATH/NUMBERS
# ============================================================
def extract_math_and_numbers(text: str) -> Dict[str, Any]:
    """
    Extract math symbols and numbers separately from text
    Returns: {"math_content": "", "numeric_content": "", "pure_text": ""}
    """
    if not text:
        return {"math_content": "", "numeric_content": "", "pure_text": ""}
    
    # Extract math symbols
    math_symbols = r'[+\-*/=¬±√ó√∑‚â†‚âà‚â°‚â§‚â•<>‚Üí‚Üê‚Üë‚Üì‚Üî‚àù‚àû‚àÇ‚àÜ‚àá‚àö‚àõ‚àú‚à´‚à¨‚à≠‚àÆ‚àØ‚à∞‚àë‚àè‚àê‚àß‚à®‚à©‚à™‚àà‚àâ‚äÇ‚äÉ‚äÜ‚äá¬¨‚àÄ‚àÉ‚àÑ‚àÖ]'
    math_content = ' '.join(re.findall(math_symbols, text))
    
    # Extract numbers and numeric patterns
    numeric_patterns = [
        r'\b\d+\b',  # standalone numbers
        r'\d+\.\d+',  # decimals
        r'\d+/\d+',   # fractions
        r'\d+\s*[\+\-\*\/]\s*\d+',  # simple equations,
    ]
    
    numeric_content = []
    for pattern in numeric_patterns:
        numeric_content.extend(re.findall(pattern, text))
    numeric_content = ' '.join(numeric_content)
    
    # Extract pure text (remove math and numbers)
    pure_text = text
    # Remove math symbols
    pure_text = re.sub(math_symbols, '', pure_text)
    # Remove numbers
    pure_text = re.sub(r'\b\d+\b', '', pure_text)
    pure_text = re.sub(r'\d+\.\d+', '', pure_text)
    pure_text = re.sub(r'\d+/\d+', '', pure_text)
    # Clean up
    pure_text = re.sub(r'\s+', ' ', pure_text).strip()
    
    return {
        "math_content": math_content,
        "numeric_content": numeric_content, 
        "pure_text": pure_text
    }

def should_separate_math_numbers(text: str) -> bool:
    """
    Check if text contains math symbols or numbers that should be separated
    """
    if not text:
        return False
    
    # Check for math symbols
    math_symbols = r'[+\-*/=¬±√ó√∑‚â†‚âà‚â°‚â§‚â•<>‚Üí‚Üê‚Üë‚Üì‚Üî‚àù‚àû‚àÇ‚àÜ‚àá‚àö‚àõ‚àú‚à´‚à¨‚à≠‚àÆ‚àØ‚à∞‚àë‚àè‚àê‚àß‚à®‚à©‚à™‚àà‚àâ‚äÇ‚äÉ‚äÜ‚äá¬¨‚àÄ‚àÉ‚àÑ‚àÖ]'
    if re.search(math_symbols, text):
        return True
    
    # Check for numeric content
    numeric_patterns = [
        r'\b\d{2,}\b',  # numbers with 2+ digits
        r'\d+\.\d+',    # decimals
        r'\d+/\d+',     # fractions
        r'\d+\s*[\+\-\*\/]\s*\d+',  # equations
    ]
    
    for pattern in numeric_patterns:
        if re.search(pattern, text):
            return True
    
    return False

def enhanced_clean_extracted_text(text: str) -> str:
    """Comprehensive OCR correction while preserving special content"""
    if not text:
        return text
    
    # Enhanced OCR corrections
    replacements = {
        'wi‚àö': 'will', 'fi‚àö': 'fill', 'studesnts': 'students',
        'shk': 'CS', '‚àö': 'square root', '√∑': '√∑', '√ó': '√ó',
        '¬≤': '¬≤', '¬≥': '¬≥', '¬∞': '¬∞', 'Ô¨Å': 'fi', 'Ô¨Ç': 'fl'
    }
    
    for wrong, correct in replacements.items():
        text = text.replace(wrong, correct)
    
    # Fix common OCR issues with regex
    text = re.sub(r'(\d)\s+(\d)', r'\1\2', text)  # Fix spaced numbers
    text = re.sub(r'(\w)-\s+(\w)', r'\1-\2', text)  # Fix hyphenated words
    
    return text.strip()


def merge_fragmented_text_blocks(blocks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """Merge single-character text blocks into proper words"""
    if not blocks:
        return blocks
    
    merged_blocks = []
    current_block = None
    
    for block in blocks:
        content = block.get('content', '').strip()
        
        # Skip if content is empty after filtering
        if not content:
            continue
            
        # Check if this is a fragmented character (single char with small font)
        is_fragment = (len(content) == 1 and 
                      block.get('font', {}).get('size', 12) < 8 and
                      content.isalpha())
        
        if is_fragment:
            if current_block:
                # Merge with current block
                current_block['content'] += content
                # Update position to include the merged character
                current_block['position']['x1'] = block['position']['x1']
                current_block['position']['width'] = current_block['position']['x1'] - current_block['position']['x0']
                current_block['word_count'] = len(current_block['content'].split())
            else:
                # Start new merged block
                current_block = block.copy()
        else:
            # Non-fragment block
            if current_block:
                merged_blocks.append(current_block)
                current_block = None
            merged_blocks.append(block)
    
    # Don't forget the last merged block
    if current_block:
        merged_blocks.append(current_block)
    
    return merged_blocks


def improved_content_classification(text: str) -> str:
    """Better content classification for government exam papers"""
    # Check for Telugu text first (preserve it)
    telugu_chars = re.findall(r'[\u0C00-\u0C7F]', text)
    if telugu_chars:
        return "telugu_text"
    
    text_lower = text.lower().strip()
    
    # Instructions
    if any(word in text_lower for word in ['directions', 'study the', 'answer the', 'following questions', 'instruction']):
        return "instruction"
    
    # Questions (starts with number and period)
    elif re.match(r'^\d+\.', text.strip()):
        return "question"
    
    # Options (starts with number and parenthesis)
    elif re.match(r'^[1-5]\)', text.strip()):
        return "option"
    
    # Data labels (years, numbers on axes)
    elif re.match(r'^(201[2-9]|202[0-9]|60|50|40|30|20|10|0)$', text.strip()):
        return "data_label"
    
    # Headers (all caps or title case)
    elif re.match(r'^[A-Z][A-Z\s]+$', text.strip()) or text_lower in ['numerical ability', 'years', 'question', 'answer']:
        return "header"
    
    # Mathematical content
    elif any(symbol in text for symbol in ['√∑', '√ó', '=', '%', '‚àö', '¬≤', '¬≥', 'square root']):
        return "mathematical"
    
    # Exam names and acronyms
    elif any(acronym in text for acronym in ['SBI', 'LIC', 'NIACL', 'MTS', 'CGL', 'CHSL', 'RRB', 'IBPS']):
        return "exam_reference"
    
    else:
        return "normal_text"


# ============================================================
#  ENHANCED MATH CONTENT DETECTION AND PROCESSING
# ============================================================
def is_full_math_content(text: str) -> bool:
    """
    Detect if text content is predominantly mathematical
    """
    if not text or len(text.strip()) < 2:
        return False
    
    # Clean the text for analysis
    clean_text = text.strip()
    
    # Enhanced math symbols detection - include question marks and parentheses
    math_symbols = re.findall(r'[√∑√ó=‚àö¬≤¬≥‚àõ‚àú\+\-\*\/\%\?\(\)]', clean_text)
    numbers = re.findall(r'\b\d+\b', clean_text)
    
    # Count meaningful text words (at least 2 letters)
    meaningful_words = re.findall(r'\b[a-zA-Z]{2,}\b', clean_text)
    
    # Calculate ratios
    total_math_elements = len(math_symbols) + len(numbers)
    total_text_elements = len(meaningful_words)
    total_elements = total_math_elements + total_text_elements
    
    # Avoid division by zero
    if total_elements == 0:
        return False
    
    math_ratio = total_math_elements / total_elements
    
    # Heuristic 1: If math elements make up 80% or more of content
    if math_ratio >= 0.8:
        return True
    
    # Heuristic 2: If there are 4+ math elements and very few words
    if total_math_elements >= 4 and total_text_elements <= 2:
        return True
    
    # Heuristic 3: Enhanced mathematical patterns
    math_patterns = [
        r'\d+/\d+',  # Fractions like 25/32
        r'\d+\s*\%',  # Percentages like 35%
        r'\d+\s*[\+\-\*\/]\s*\d+',  # Basic operations
        r'.*=\s*\?',  # Equations with unknowns
        r'\(.*[\+\-\*\/].*\)',  # Operations in parentheses
        r'.*√∑.*√ó.*',  # Multiple operations
    ]
    
    pattern_count = sum(1 for pattern in math_patterns if re.search(pattern, clean_text))
    if pattern_count >= 2:
        return True
    
    # Heuristic 4: Pure math expressions (no meaningful words, but has math operations)
    if len(meaningful_words) == 0 and len(math_symbols) >= 3:
        return True
    
    return False

def change_mathematical_to_normal_text(json_path: str, output_path: str = None):
    """
    Change all 'content_type': 'mathematical' to 'normal_text' in the JSON file
    AND detect full math content to change to 'mathematical'
    """
    if output_path is None:
        output_path = json_path
    
    print(f"üîÑ Processing content types...")
    
    with open(json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    math_to_normal_count = 0
    detected_math_count = 0
    
    for page in data.get('pages', []):
        # Process text_content blocks
        for text_block in page.get('text_content', []):
            content = text_block.get('content', '')
            current_type = text_block.get('content_type')
            
            # 1. Change existing mathematical to normal_text
            if current_type == 'mathematical':
                text_block['content_type'] = 'normal_text'
                math_to_normal_count += 1
            
            # 2. Detect full math content and change to mathematical
            elif is_full_math_content(content):
                text_block['content_type'] = 'mathematical'
                detected_math_count += 1
                print(f"   üî¢ Detected math content: '{content[:50]}...'")
    
    # Save the modified data
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=2, ensure_ascii=False)
    
    print(f"‚úÖ Changed {math_to_normal_count} mathematical ‚Üí normal_text")
    print(f"‚úÖ Detected {detected_math_count} full math content ‚Üí mathematical")
    
    return data


# ============================================================
#  ENHANCED BAR CHART DETECTION
# ============================================================
class EnhancedBarChartDetector:
    def __init__(self):
        self.chart_keywords = [
            'bar chart', 'students registered', 'exams', 'years', 'data interpretation',
            'vertical bar', 'horizontal bar', 'column chart', 'graph', 'diagram'
        ]
    
    def detect_bar_chart_enhanced(self, image_info, page_text_blocks, page_num):
        """Enhanced bar chart detection with contextual analysis"""
        detection_score = 0
        detection_features = []
        
        # Feature 1: Image dimension analysis (30% weight)
        width = image_info['dimensions']['width']
        height = image_info['dimensions']['height']
        aspect_ratio = width / height if height > 0 else 0
        
        if 1.2 <= aspect_ratio <= 3.0:  # Typical bar chart ratios
            detection_score += 0.3
            detection_features.append('optimal_aspect_ratio')
        
        # Feature 2: Contextual text analysis (40% weight)
        contextual_score = self._analyze_surrounding_context(page_text_blocks)
        detection_score += contextual_score * 0.4
        if contextual_score > 0.5:
            detection_features.append('strong_contextual_evidence')
        
        # Feature 3: Size-based confidence (30% weight)
        if width > 300 and height > 150:  # Significant chart size
            detection_score += 0.3
            detection_features.append('adequate_size')
        
        # Feature 4: Page position analysis (bonus)
        if self._is_typical_chart_position(image_info['position']):
            detection_score += 0.1
            detection_features.append('typical_position')
        
        return {
            'is_bar_chart_candidate': detection_score >= 0.6,
            'confidence': min(detection_score, 1.0),
            'detection_features': detection_features,
            'chart_type': self._classify_chart_type(aspect_ratio, contextual_score),
            'extraction_priority': 'high' if detection_score > 0.7 else 'medium'
        }
    
    def _analyze_surrounding_context(self, text_blocks):
        """Analyze surrounding text for chart indicators"""
        context_score = 0
        relevant_text = ""
        
        for block in text_blocks:
            content = block.get('content', '').lower()
            relevant_text += " " + content
            
            # Direct chart mentions
            if any(keyword in content for keyword in self.chart_keywords):
                context_score += 0.3
            
            # Data interpretation patterns
            if any(pattern in content for pattern in ['study the', 'following data', 'answer the questions']):
                context_score += 0.2
            
            # Year patterns (common in charts)
            if re.search(r'20[0-9]{2}', content):
                context_score += 0.1
        
        # Check for exam acronyms in context
        if any(acronym in relevant_text for acronym in ['mts', 'cgl', 'chsl']):
            context_score += 0.2
        
        return min(context_score, 1.0)
    
    def _is_typical_chart_position(self, position):
        """Check if image is in typical chart position"""
        x0, y0 = position.get('x0', 0), position.get('y0', 0)
        # Charts are typically centered or top-half of page
        return x0 > 100 and y0 > 200
    
    def _classify_chart_type(self, aspect_ratio, contextual_score):
        """Classify chart type based on features"""
        if aspect_ratio > 2.0:
            return "vertical_bar_chart"
        elif aspect_ratio < 1.0:
            return "horizontal_bar_chart"
        else:
            return "grouped_bar_chart"


def extract_axis_labels_from_context(page_text_blocks, chart_position):
    """Extract axis labels from surrounding text blocks"""
    axis_data = {'x_labels': [], 'y_labels': [], 'title': ''}
    
    for block in page_text_blocks:
        block_pos = block.get('position', {})
        content = block.get('content', '').strip()
        
        # Check if text block is near the chart (simplified proximity check)
        chart_x, chart_y = chart_position.get('x0', 0), chart_position.get('y0', 0)
        block_x, block_y = block_pos.get('x0', 0), block_pos.get('y0', 0)
        
        if abs(block_x - chart_x) < 200 and abs(block_y - chart_y) < 200:
            
            # Extract X-axis labels (years: 2012, 2013, etc.)
            year_matches = re.findall(r'20[0-9]{2}', content)
            if year_matches:
                axis_data['x_labels'] = sorted(set(year_matches))
                print(f"üìÖ Extracted X-axis years: {axis_data['x_labels']}")
            
            # Extract Y-axis labels (numbers: 0, 10, 20, etc.)
            number_matches = re.findall(r'\b\d{1,3}\b', content)
            if len(number_matches) >= 3 and all(int(n) % 10 == 0 for n in number_matches[:3]):
                axis_data['y_labels'] = sorted(set(int(n) for n in number_matches))
                print(f"üìä Extracted Y-axis scale: {axis_data['y_labels']}")
            
            # Extract chart title
            if len(content) > 20 and len(content) < 100 and not content.startswith('1)'):
                axis_data['title'] = content
                print(f"üè∑Ô∏è Extracted chart title: {content[:50]}...")
    
    return axis_data


# ============================================================
#  ENHANCED ERROR HANDLING
# ============================================================
class GracefulDegradation:
    def __init__(self):
        self.fallback_strategies = {
            'text_extraction': self._fallback_text_extraction,
            'image_processing': self._fallback_image_processing,
            'table_parsing': self._fallback_table_parsing
        }
        self.error_log = []
    
    def handle_extraction_error(self, operation, error, context):
        """Comprehensive error handling with graceful degradation"""
        error_id = f"ERR_{hash(str(error))[:8]}"
        
        # Log error with context
        self._log_error(error_id, operation, error, context)
        
        # Try fallback strategy
        fallback_result = self._attempt_fallback(operation, context)
        
        if fallback_result['success']:
            return {
                'success': True,
                'data': fallback_result['data'],
                'quality': 'degraded',
                'error_handled': error_id,
                'fallback_used': fallback_result['method']
            }
        else:
            return {
                'success': False,
                'error': f"Operation failed: {str(error)}",
                'error_id': error_id,
                'suggested_action': self._get_recovery_suggestion(operation)
            }
    
    def _attempt_fallback(self, operation, context):
        """Attempt fallback strategies for failed operations"""
        fallback_method = self.fallback_strategies.get(operation)
        
        if fallback_method:
            try:
                result = fallback_method(context)
                return {'success': True, 'data': result, 'method': operation}
            except Exception as fallback_error:
                print(f"‚ö†Ô∏è Fallback also failed: {fallback_error}")
        
        return {'success': False, 'method': 'none_available'}
    
    def _fallback_text_extraction(self, context):
        """Fallback for text extraction failures"""
        print("üîÑ Using fallback text extraction...")
        # Simplified text extraction as fallback
        return {"text_blocks": [], "method": "fallback_simple"}
    
    def _fallback_image_processing(self, context):
        """Fallback for image processing failures"""
        print("üîÑ Using fallback image processing...")
        return {"images": [], "method": "fallback_basic"}
    
    def _fallback_table_parsing(self, context):
        """Fallback for table parsing failures"""
        print("üîÑ Using fallback table parsing...")
        return {"tables": [], "method": "fallback_simple"}
    
    def _log_error(self, error_id, operation, error, context):
        """Log error with context"""
        error_entry = {
            'error_id': error_id,
            'timestamp': datetime.now().isoformat(),
            'operation': operation,
            'error_type': type(error).__name__,
            'error_message': str(error),
            'context': str(context)[:200]  # Limit context length
        }
        self.error_log.append(error_entry)
        print(f"üìù Error logged: {error_id} - {operation} - {error}")
    
    def _get_recovery_suggestion(self, operation):
        """Get recovery suggestions based on operation type"""
        suggestions = {
            'text_extraction': 'Try adjusting OCR settings or preprocessing PDF',
            'image_processing': 'Reduce image quality or skip image extraction',
            'table_parsing': 'Use simpler table detection algorithm',
            'default': 'Check PDF quality and try alternative extraction methods'
        }
        return suggestions.get(operation, suggestions['default'])
    
    def generate_error_report(self):
        """Generate error analytics report"""
        if not self.error_log:
            return {"status": "No errors recorded", "health_score": 1.0}
        
        total_errors = len(self.error_log)
        error_types = {}
        
        for error in self.error_log:
            error_type = error['error_type']
            error_types[error_type] = error_types.get(error_type, 0) + 1
        
        return {
            "total_errors": total_errors,
            "error_distribution": error_types,
            "health_score": max(0, 1 - (total_errors / 100)),
            "recent_errors": self.error_log[-5:] if self.error_log else []
        }


# ============================================================
#  VECTOR MATH SYMBOL DETECTION
# ============================================================
def detect_math_drawings(pdf_path: str) -> dict:
    """Detect vector-drawn math shapes with CORRECT coordinates"""
    detected = {}
    doc = fitz.open(pdf_path)
    print(f"\nüîç Scanning {pdf_path} for vector math symbols...")
    
    for page_num, page in enumerate(doc):
        page_height = page.rect.height  # Get actual page height
        drawings = page.get_drawings()
        math_like = []
        
        for d in drawings:
            bbox = d.get("rect", None)
            if not bbox:
                continue
                
            width = abs(bbox[2] - bbox[0])
            height = abs(bbox[3] - bbox[1])
            aspect = width / max(height, 1)
            
            # FIX: Convert PyMuPDF coordinates to standard PDF (Y=0 at bottom)
            fixed_y0 = page_height - bbox[3]  # Convert Y coordinates
            fixed_y1 = page_height - bbox[1]
            
            # Heuristic for math lines / bars / radicals
            if (height < 3 and width > 10) or (aspect < 0.2 and height > 10):
                math_like.append({
                    "type": "vector_symbol",
                    "symbol_guess": "fraction_bar" if height < 3 else "radical_or_vertical",
                    "bbox": [bbox[0], fixed_y0, bbox[2], fixed_y1],  # FIXED coordinates
                    "dimensions": {"width": width, "height": height},
                    "source": "drawing-stroke",
                    "page": page_num
                })
                
        if math_like:
            detected[page_num] = math_like
            print(f"üìê Page {page_num + 1}: {len(math_like)} math symbols with FIXED coordinates")
            
    doc.close()
    return detected


def merge_math_symbols_into_json(json_path: str, detected_math: dict):
    """Merge detected math symbols directly into the same JSON (no new file)"""
    if not detected_math:
        print("‚ÑπÔ∏è No vector math symbols found.")
        return

    with open(json_path, "r", encoding="utf-8") as f:
        data = json.load(f)

    for page in data.get("pages", []):
        page_index = page.get("page_number", 1) - 1
        if page_index in detected_math:
            page["vector_symbols"] = detected_math[page_index]

    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=2, ensure_ascii=False)
    print(f"‚úÖ Math symbols merged into JSON: {json_path}")


# ============================================================
#  TRANSLATION HELPER
# ============================================================
def translate_json_preserve_structure(extracted_json_path: str, translated_json_path: str, target_lang: str = 'te'):
    """
    Translate text content while PRESERVING vector symbols and layout elements
    """
    print(f"\nüåê Translating JSON while preserving structure...")
    print(f"   Source: {extracted_json_path}")
    print(f"   Target: {translated_json_path}")
    print(f"   Language: {target_lang}")
    
    with open(extracted_json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # Count elements before translation for verification
    original_vector_count = sum(len(page.get('vector_symbols', [])) for page in data['pages'])
    original_layout_count = sum(len(page.get('layout_elements', [])) for page in data['pages'])
    
    print(f"   Original vector symbols: {original_vector_count}")
    print(f"   Original layout elements: {original_layout_count}")
    
    # Translate only the text content that should be translated
    translated_blocks = 0
    for page in data['pages']:
        for text_block in page['text_content']:
            if text_block.get('translation_ready', True) and text_block.get('content_type') != 'mathematical':
                original_text = text_block['content']
                
                # ‚≠ê‚≠ê SIMULATE TRANSLATION - REPLACE WITH REAL TRANSLATION API ‚≠ê‚≠ê
                if target_lang == 'te':  # Telugu
                    # This is a placeholder - replace with actual translation
                    translated_text = f"[Telugu: {original_text[:50]}...]"
                else:
                    translated_text = original_text
                
                text_block['translated_content'] = translated_text
                translated_blocks += 1
    
    print(f"   Translated text blocks: {translated_blocks}")
    
    # ‚≠ê‚≠ê‚≠ê CRITICAL: Verify vector symbols and layout elements are preserved ‚≠ê‚≠ê‚≠ê
    final_vector_count = sum(len(page.get('vector_symbols', [])) for page in data['pages'])
    final_layout_count = sum(len(page.get('layout_elements', [])) for page in data['pages'])
    
    print(f"   Final vector symbols: {final_vector_count}")
    print(f"   Final layout elements: {final_layout_count}")
    
    if final_vector_count == original_vector_count and final_layout_count == original_layout_count:
        print("‚úÖ SUCCESS: All vector symbols and layout elements preserved!")
    else:
        print("‚ùå WARNING: Some elements may have been lost during translation!")
    
    # Save translated JSON
    with open(translated_json_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=2, ensure_ascii=False)
    
    print(f"‚úÖ Translated JSON saved: {translated_json_path}")
    return data


# ============================================================
#  JSON VERIFICATION FUNCTION
# ============================================================
def verify_json_structure(json_path: str, expected_vector_count: int = 82, expected_layout_count: int = 77):
    """Verify that JSON contains all necessary elements for PDF generation"""
    print(f"\nüîç Verifying JSON structure: {json_path}")
    
    with open(json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    total_vector_symbols = 0
    total_layout_lines = 0
    total_text_blocks = 0
    total_images = 0
    total_math_blocks = 0
    
    for page_num, page in enumerate(data.get('pages', [])):
        vector_symbols = page.get('vector_symbols', [])
        layout_elements = page.get('layout_elements', [])
        text_content = page.get('text_content', [])
        images = page.get('images', [])
        math_content = page.get('math_numeric_content', [])
        
        total_vector_symbols += len(vector_symbols)
        total_layout_lines += len([e for e in layout_elements if e.get('type') == 'line'])
        total_text_blocks += len(text_content)
        total_images += len(images)
        total_math_blocks += len(math_content)
    
    print(f"üìä JSON Content Summary:")
    print(f"   ‚Ä¢ Vector symbols: {total_vector_symbols} (expected: {expected_vector_count})")
    print(f"   ‚Ä¢ Layout lines: {total_layout_lines} (expected: {expected_layout_count})")
    print(f"   ‚Ä¢ Text blocks: {total_text_blocks}")
    print(f"   ‚Ä¢ Math/numeric blocks: {total_math_blocks}")
    print(f"   ‚Ä¢ Images: {total_images}")
    
    # Check if ready for PDF generation
    has_vector_symbols = total_vector_symbols > 0
    has_layout_elements = total_layout_lines > 0
    has_text_content = total_text_blocks > 0
    
    if has_vector_symbols and has_layout_elements and has_text_content:
        print("‚úÖ READY for PDF generation - All elements present!")
        return True
    else:
        print("‚ùå NOT READY for PDF generation - Missing elements!")
        if not has_vector_symbols:
            print("   - Missing vector symbols")
        if not has_layout_elements:
            print("   - Missing layout elements") 
        if not has_text_content:
            print("   - Missing text content")
        return False


# ============================================================
#  FIXED CONTENT FILTERING FUNCTIONS
# ============================================================
# ============================================================
#  MAIN PDF ‚Üí JSON CONVERTER CLASS - FIXED VERSION
# ============================================================
class PDFToJSONConverter:
    def __init__(self):
        self.supported_formats = ['.pdf']
        self.bar_chart_detector = EnhancedBarChartDetector()
        self.error_handler = GracefulDegradation()

    def _is_pure_numeric_pattern(self, text: str) -> bool:
        """
        Detect pure numeric/option patterns that should go to MATH BLOCKS
        """
        if not text or len(text.strip()) == 0:
            return False
        
        clean_text = text.strip()
        
        # Pure option patterns: "1) 11:15", "A) 25", "1)445", etc.
        if re.match(r'^(\d+|[A-D])\)\s*\d+[:]?\d*$', clean_text):
            return True
        
        # Pure ratios: "11:15", "9:17" 
        if re.match(r'^\d+[:]\d+$', clean_text):
            return True
        
        # Pure number sequences (3+ numbers)
        numbers = re.findall(r'\b\d+\b', clean_text)
        non_numeric = re.sub(r'\b\d+\b', '', clean_text).strip()
        if len(numbers) >= 3 and len(non_numeric) == 0:
            return True
        
        # Pure numbers with basic operators
        if re.match(r'^\d+\s*[\+\-\*\/]\s*\d+\s*=\s*\d+$', clean_text):
            return True
            
        return False

    def _is_translatable_content(self, text: str) -> bool:
        """Determine if text block should be translated - ENHANCED VERSION"""
        if not text or len(text.strip()) < 2:
            return False
        
        # Don't translate Telugu text
        telugu_chars = re.findall(r'[\u0C00-\u0C7F]', text)
        if telugu_chars:
            return False
        
        # ‚≠ê‚≠ê FIXED: Use the same logic as should_translate_content ‚≠ê‚≠ê
        # Don't translate pure options like "1) 445", "2) 556"
        if re.match(r'^[1-5]\)\s*\d+$', text.strip()):
            return False
        
        # Don't translate pure number sequences
        if re.match(r'^\d+\s+\d+\s+\d+\s+\d+\s+\d+\s+\?$', text.strip()):
            return False
        
        # Don't translate acronyms
        acronyms = ['SBI', 'LIC', 'NIACL', 'MTS', 'CGL', 'CHSL', 'CCE', 'RRB', 'IBPS']
        if all(word in acronyms or word.isdigit() for word in text.upper().split()):
            return False
        
        # ‚≠ê‚≠ê CRITICAL FIX: Check if it's PURE math/numeric (no meaningful text)
        # Count meaningful words (at least 2 letters)
        words = re.findall(r'\b[a-zA-Z]{2,}\b', text)
        numbers = re.findall(r'\b\d+\b', text)
        math_symbols = re.findall(r'[√∑√ó=‚àö¬≤¬≥‚àõ‚àú]', text)  # Pure math symbols
        
        # If NO meaningful words AND has numbers/math symbols, it's pure math ‚Üí DON'T TRANSLATE
        if len(words) == 0 and (len(numbers) > 0 or len(math_symbols) > 0):
            return False
        
        # If it's mostly numbers with very little text (1 word vs 2+ numbers)
        if len(words) <= 1 and len(numbers) >= 2:
            return False
        
        # Complex math expressions with multiple symbols
        if len(math_symbols) >= 2:
            return False
        
        return True

    def _classify_content_type(self, text: str) -> str:
        """Enhanced content classification"""
        return improved_content_classification(text)

    # ============================================================
    #  FIXED METHODS - THE REAL SOLUTION
    # ============================================================

    def _extract_text_enhanced(self, page, coord_converter) -> List[Dict[str, Any]]:
        """Enhanced text extraction - ONLY meaningful text content"""
        text_blocks = []
        try:
            words = page.extract_words(
                extra_attrs=["fontname", "size"], 
                x_tolerance=1.5,
                y_tolerance=2,
                keep_blank_chars=False,
                use_text_flow=True
            )
            
            current_block, current_y, y_tol = [], None, 5
            for w in sorted(words, key=lambda w: (w['top'], w['x0'])):
                original_text = w['text'].strip()
                
                # ‚≠ê‚≠ê FIXED: Check the COMBINED block content, not individual words
                # We'll check after building the block
                cleaned_word = w.copy()
                cleaned_word['text'] = original_text
                
                if current_y is None or abs(w['top'] - current_y) > y_tol:
                    if current_block:
                        # ‚≠ê‚≠ê KEY FIX: Check if the complete block should be math
                        block_text = ' '.join(word['text'] for word in current_block)
                        if self._is_pure_numeric_pattern(block_text):
                            # Don't add to text_blocks - it will go to math blocks
                            pass
                        else:
                            text_blocks.append(self._create_text_block_enhanced(current_block, coord_converter))
                    current_block, current_y = [cleaned_word], w['top']
                else:
                    current_block.append(cleaned_word)
                    
            if current_block:
                # ‚≠ê‚≠ê KEY FIX: Check the final block too
                block_text = ' '.join(word['text'] for word in current_block)
                if self._is_pure_numeric_pattern(block_text):
                    # Don't add to text_blocks - it will go to math blocks
                    pass
                else:
                    text_blocks.append(self._create_text_block_enhanced(current_block, coord_converter))
                
        except Exception as e:
            print(f"    ‚ö†Ô∏è Enhanced text extraction failed: {e}")
            # Fallback to basic extraction
            try:
                text = page.extract_text()
                if text:
                    text_blocks.append({
                        "type": "text_block", 
                        "content": text,
                        "position": {"x0": 0, "y0": 0, "x1": float(page.width), "y1": float(page.height)},
                        "font": {"name": "", "size": 0},
                        "word_count": len(text.split()),
                        "translation_ready": self._is_translatable_content(text),
                        "content_type": self._classify_content_type(text),
                        "extraction_method": "fallback_basic"
                    })
            except:
                pass
                
        return text_blocks

    def _extract_math_content_enhanced(self, page, coord_converter) -> List[Dict[str, Any]]:
        """Extract PURE math symbols and numeric patterns - FIXED VERSION"""
        math_blocks = []
        
        try:
            words = page.extract_words(
                extra_attrs=["fontname", "size"], 
                x_tolerance=1.5,
                y_tolerance=2,
                keep_blank_chars=False,
                use_text_flow=True
            )
            
            # ‚≠ê‚≠ê KEY FIX: Group words into blocks first, then check patterns
            current_block, current_y, y_tol = [], None, 5
            
            for w in sorted(words, key=lambda w: (w['top'], w['x0'])):
                if current_y is None or abs(w['top'] - current_y) > y_tol:
                    if current_block:
                        # Check if the complete block is a pure numeric pattern
                        block_text = ' '.join(word['text'] for word in current_block)
                        if self._is_pure_numeric_pattern(block_text):
                            # Create math block from the combined words
                            math_block = self._create_math_block_from_words(current_block, coord_converter)
                            if math_block:
                                math_blocks.append(math_block)
                    current_block, current_y = [w], w['top']
                else:
                    current_block.append(w)
                    
            # Process the final block
            if current_block:
                block_text = ' '.join(word['text'] for word in current_block)
                if self._is_pure_numeric_pattern(block_text):
                    math_block = self._create_math_block_from_words(current_block, coord_converter)
                    if math_block:
                        math_blocks.append(math_block)
            
            # Also include single math symbols
            for w in words:
                original_text = w['text'].strip()
                if len(original_text) == 1 and original_text in '√∑√ó=+-*/':
                    extracted = extract_math_and_numbers(original_text)
                    math_block = self._create_math_block_enhanced(w, extracted, coord_converter)
                    if math_block:
                        math_blocks.append(math_block)
                        
        except Exception as e:
            print(f"    ‚ö†Ô∏è Math content extraction failed: {e}")
            
        return math_blocks

    def _create_math_block_from_words(self, words: List[Dict[str, Any]], coord_converter):
        """Create math block from multiple words"""
        if not words:
            return None
            
        # Combine all words into one text
        combined_text = ' '.join(w['text'] for w in words)
        extracted = extract_math_and_numbers(combined_text)
        
        # Calculate combined position
        x0 = min(w['x0'] for w in words)
        top = min(w['top'] for w in words)
        x1 = max(w['x1'] for w in words)
        bottom = max(w['bottom'] for w in words)
        
        # Convert coordinates
        fixed_top = coord_converter.pdfplumber_to_standard(bottom)
        fixed_bottom = coord_converter.pdfplumber_to_standard(top)
        
        return {
            "type": "math_numeric_content",
            "original_content": combined_text,
            "math_symbols": extracted["math_content"],
            "numeric_content": extracted["numeric_content"],
            "position": {
                "x0": float(x0), "y0": float(fixed_top),
                "x1": float(x1), "y1": float(fixed_bottom),
                "width": float(x1 - x0), "height": float(bottom - top)
            },
            "font": {
                "name": words[0].get('fontname', ''),
                "size": float(words[0].get('size', 0)),
            },
            "extraction_method": "grouped_math_pattern",
            "translation_ready": False,
            "content_type": "mathematical" if extracted["math_content"] else "numeric"
        }

    def _create_math_block_enhanced(self, word_data, extracted_content, coord_converter):
        """Create separate block for PURE math symbols and numeric patterns"""
        if not extracted_content["math_content"] and not extracted_content["numeric_content"]:
            return None
        
        x0, top = word_data['x0'], word_data['top']
        x1, bottom = word_data['x1'], word_data['bottom']
        
        # Convert coordinates
        fixed_top = coord_converter.pdfplumber_to_standard(bottom)
        fixed_bottom = coord_converter.pdfplumber_to_standard(top)
        
        return {
            "type": "math_numeric_content",
            "original_content": word_data['text'],
            "math_symbols": extracted_content["math_content"],
            "numeric_content": extracted_content["numeric_content"],
            "position": {
                "x0": float(x0), "y0": float(fixed_top),
                "x1": float(x1), "y1": float(fixed_bottom),
                "width": float(x1 - x0), "height": float(bottom - top)
            },
            "font": {
                "name": word_data.get('fontname', ''),
                "size": float(word_data.get('size', 0)),
            },
            "extraction_method": "separated_math_numeric",
            "translation_ready": False,  # Never translate pure math
            "content_type": "mathematical" if extracted_content["math_content"] else "numeric"
        }

    def _create_text_block_enhanced(self, words: List[Dict[str, Any]], coord_converter) -> Dict[str, Any]:
        """Create text block with consistent coordinates"""
        if not words:
            return {}
            
        x0, top = min(w['x0'] for w in words), min(w['top'] for w in words)
        x1, bottom = max(w['x1'] for w in words), max(w['bottom'] for w in words)
        text_content = ' '.join(w['text'] for w in words)
        fonts = [w.get('fontname', '') for w in words if w.get('fontname')]
        font_size = words[0].get('size', 0) if words else 0
        
        # Use coordinate converter for consistent coordinates
        fixed_top = coord_converter.pdfplumber_to_standard(bottom)
        fixed_bottom = coord_converter.pdfplumber_to_standard(top)
        
        content_type = self._classify_content_type(text_content)
        should_translate = self._is_translatable_content(text_content)
        
        return {
            "type": "text_block",
            "content": text_content,
            "position": {
                "x0": float(x0), "y0": float(fixed_top),
                "x1": float(x1), "y1": float(fixed_bottom),
                "width": float(x1 - x0), "height": float(bottom - top)
            },
            "font": {
                "name": max(set(fonts), key=fonts.count) if fonts else "", 
                "size": float(font_size),
                "bold": any('bold' in f.lower() for f in fonts),
                "italic": any('italic' in f.lower() for f in fonts)
            },
            "word_count": len(words),
            "translation_ready": should_translate,
            "content_type": content_type,
            "extraction_method": "enhanced"
        }

    # ============================================================
    #  EXISTING METHODS (unchanged)
    # ============================================================

    def convert_pdf_to_json_enhanced(self, pdf_path: str, output_path: str = None,
                                    include_images: bool = False,
                                    image_handling: str = "metadata") -> Dict[str, Any]:
        """Enhanced PDF to JSON conversion - CORRECT CONTENT SEPARATION"""
        
        if not Path(pdf_path).exists():
            raise FileNotFoundError(f"PDF file not found: {pdf_path}")

        print(f"üöÄ FIXED PDF Conversion: {Path(pdf_path).name}")
        print(f"   Image handling: {image_handling}")
        print(f"   Include images: {include_images}")

        # Single PDF opening for all operations
        with pdfplumber.open(pdf_path) as pdf:
            fitz_doc = fitz.open(pdf_path)
            
            pdf_data = {
                "metadata": self._extract_enhanced_metadata(pdf_path, pdf),
                "pages": [],
                "extraction_settings": {
                    "coordinate_system": "standard_pdf_y0_at_bottom",
                    "units": "points", 
                    "text_cleaning": "enhanced_ocr_correction",
                    "content_separation": "pure_numeric_to_math_blocks",
                    "image_handling": image_handling,
                    "include_images": include_images,
                    "timestamp": datetime.now().isoformat()
                }
            }
            
            for page_num, page in enumerate(pdf.pages):
                print(f"  üìÑ Processing page {page_num + 1}/{len(pdf.pages)}...")
                
                # Unified coordinate converter for this page
                coord_converter = CoordinateConverter(page.height)
                
                # Extract text and math separately
                text_content = self._extract_text_enhanced(page, coord_converter)
                math_content = self._extract_math_content_enhanced(page, coord_converter)
                
                page_data = {
                    "page_number": page_num + 1,
                    "dimensions": {
                        "width": float(page.width), 
                        "height": float(page.height),
                        "rotation": page.rotation or 0
                    },
                    "text_content": text_content,  # ONLY meaningful text
                    "math_numeric_content": math_content,  # PURE math symbols & numeric patterns
                    "tables": self._extract_tables_enhanced(page, coord_converter),
                    "images": self._extract_images_enhanced(page, include_images, image_handling, page_num, fitz_doc, coord_converter),
                    "layout_elements": self._extract_layout_enhanced(page, coord_converter),
                    "vector_symbols": self._extract_vector_symbols_enhanced(fitz_doc, page_num, coord_converter)
                }
                
                pdf_data["pages"].append(page_data)
            
            fitz_doc.close()
        
        # Apply enhanced text processing
        pdf_data = self._apply_enhanced_text_processing(pdf_data)
        
        # Comprehensive validation
        validation = self._comprehensive_validation(pdf_data)
        pdf_data["validation"] = validation
        
        # Save with better structure
        if output_path:
            self._save_enhanced_json(pdf_data, output_path)
        
        return pdf_data

    def _extract_enhanced_metadata(self, pdf_path, pdf):
        """Extract enhanced metadata"""
        path = Path(pdf_path)
        return {
            "file_name": path.name,
            "file_size": path.stat().st_size,
            "file_format": "PDF",
            "total_pages": len(pdf.pages),
            "extraction_date": datetime.now().isoformat(),
            "pdf_version": getattr(pdf, 'metadata', {}).get('PDF', 'Unknown')
        }

    def _apply_enhanced_text_processing(self, pdf_data):
        """Apply enhanced text cleaning and merging"""
        enhanced_data = pdf_data.copy()
        
        for page in enhanced_data["pages"]:
            text_content = page.get("text_content", [])
            
            if text_content:
                # Clean and classify each block
                cleaned_blocks = []
                for block in text_content:
                    cleaned_block = block.copy()
                    original_content = block.get("content", "")
                    cleaned_content = enhanced_clean_extracted_text(original_content)
                    cleaned_block["content"] = cleaned_content
                    
                    improved_type = improved_content_classification(cleaned_content)
                    cleaned_block["content_type"] = improved_type
                    
                    # Update translation readiness
                    if improved_type in ["telugu_text", "mathematical", "data_label", "exam_reference"]:
                        cleaned_block["translation_ready"] = False
                    
                    cleaned_blocks.append(cleaned_block)
                
                # Merge fragmented blocks
                merged_blocks = merge_fragmented_text_blocks(cleaned_blocks)
                page["text_content"] = merged_blocks
                
                # Log improvements
                original_count = len(text_content)
                merged_count = len(merged_blocks)
                if merged_count < original_count:
                    print(f"   ‚úÖ Merged {original_count - merged_count} fragmented text blocks")
        
        return enhanced_data

    def _comprehensive_validation(self, pdf_data):
        """Detailed validation of extracted content"""
        validation = {
            "summary": {},
            "page_breakdown": [],
            "issues": [],
            "recommendations": []
        }
        
        total_text_blocks = 0
        total_tables = 0
        total_images = 0
        total_vector_symbols = 0
        total_math_blocks = 0
        
        for page in pdf_data["pages"]:
            page_stats = {
                "page": page["page_number"],
                "text_blocks": len(page.get("text_content", [])),
                "math_blocks": len(page.get("math_numeric_content", [])),
                "tables": len(page.get("tables", [])),
                "images": len(page.get("images", [])),
                "vector_symbols": len(page.get("vector_symbols", [])),
                "quality_indicators": self._analyze_page_quality(page)
            }
            
            validation["page_breakdown"].append(page_stats)
            total_text_blocks += page_stats["text_blocks"]
            total_tables += page_stats["tables"]
            total_images += page_stats["images"]
            total_vector_symbols += page_stats["vector_symbols"]
            total_math_blocks += page_stats["math_blocks"]
        
        # Overall summary
        validation["summary"] = {
            "total_pages": len(pdf_data["pages"]),
            "total_text_blocks": total_text_blocks,
            "total_math_blocks": total_math_blocks,
            "total_tables": total_tables,
            "total_images": total_images,
            "total_vector_symbols": total_vector_symbols,
            "extraction_completeness": self._calculate_completeness_score(pdf_data)
        }
        
        # Generate recommendations
        if total_text_blocks == 0:
            validation["recommendations"].append("Enable OCR for better text extraction")
        if total_vector_symbols > 0:
            validation["recommendations"].append("Vector symbols detected - consider mathematical content processing")
        if total_tables == 0:
            validation["recommendations"].append("No tables detected - check table extraction settings")
        if total_math_blocks > 0:
            validation["recommendations"].append(f"Separated {total_math_blocks} PURE math/numeric blocks from text content")
        
        return validation

    def _analyze_page_quality(self, page):
        """Analyze quality indicators for a page"""
        text_blocks = page.get("text_content", [])
        if not text_blocks:
            return {"score": 0, "issues": ["No text content"]}
        
        issues = []
        valid_blocks = 0
        
        for block in text_blocks:
            content = block.get("content", "")
            if len(content.strip()) > 1:  # Non-empty content
                valid_blocks += 1
            else:
                issues.append("Empty text block")
        
        score = valid_blocks / len(text_blocks) if text_blocks else 0
        return {"score": score, "issues": issues, "valid_blocks": valid_blocks}

    def _calculate_completeness_score(self, pdf_data):
        """Calculate overall extraction completeness score"""
        total_pages = len(pdf_data["pages"])
        if total_pages == 0:
            return 0
        
        page_scores = []
        for page in pdf_data["pages"]:
            quality = self._analyze_page_quality(page)
            page_scores.append(quality["score"])
        
        return sum(page_scores) / len(page_scores) if page_scores else 0

    def _save_enhanced_json(self, data: Dict[str, Any], output_path: str):
        """Save JSON with better formatting and error handling"""
        try:
            # Create directory if it doesn't exist
            output_dir = Path(output_path).parent
            output_dir.mkdir(parents=True, exist_ok=True)
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False, default=str)
            
            file_size = Path(output_path).stat().st_size
            print(f"‚úÖ Enhanced JSON saved: {output_path} ({file_size / 1024:.1f} KB)")
            
        except Exception as e:
            print(f"‚ùå Failed to save enhanced JSON: {e}")
            raise

    def _extract_tables_enhanced(self, page, coord_converter) -> List[Dict[str, Any]]:
        """Extract tables with better structure detection"""
        tables = []
        try:
            table_objects = page.find_tables()
            extracted_tables = page.extract_tables()
            
            for i, (table_obj, table_data) in enumerate(zip(table_objects, extracted_tables)):
                if table_data and any(any(cell for cell in row) for row in table_data):
                    # Convert coordinates
                    fixed_y0 = coord_converter.pdfplumber_to_standard(table_obj.bbox[3])
                    fixed_y1 = coord_converter.pdfplumber_to_standard(table_obj.bbox[1])
                    
                    has_header = self._detect_table_header(table_data)
                    
                    tables.append({
                        "table_number": i + 1,
                        "position": {
                            "x0": float(table_obj.bbox[0]), "y0": float(fixed_y0),
                            "x1": float(table_obj.bbox[2]), "y1": float(fixed_y1)
                        },
                        "data": table_data,
                        "structure": {
                            "rows": len(table_data),
                            "columns": len(table_data[0]) if table_data else 0,
                            "has_header": has_header,
                            "header_row": 0 if has_header else None
                        },
                        "translation_ready": not has_header,
                        "extraction_method": "enhanced"
                    })
                    
        except Exception as e:
            print(f"    ‚ö†Ô∏è Enhanced table extraction failed: {e}")
            
        return tables

    def _detect_table_header(self, table_data):
        """Detect if table has a header row"""
        if not table_data or len(table_data) < 2:
            return False
        
        first_row = table_data[0]
        second_row = table_data[1] if len(table_data) > 1 else []
        
        # Simple heuristic: header often has text while data has numbers
        first_row_has_text = any(cell and not cell.replace('.', '').isdigit() for cell in first_row if cell)
        second_row_has_numbers = any(cell and cell.replace('.', '').isdigit() for cell in second_row if cell)
        
        return first_row_has_text and second_row_has_numbers

    def _extract_images_enhanced(self, page, include_images, image_handling, 
                                page_num, fitz_doc, coord_converter):
        """Enhanced image extraction with real coordinates and robust error handling"""
        images = []
        if not include_images or not fitz_doc:
            return images
        
        try:
            pymupdf_page = fitz_doc[page_num]
            page_height = pymupdf_page.rect.height
            image_list = pymupdf_page.get_images(full=True)
            
            print(f"    üì∑ Enhanced image extraction: found {len(image_list)} images")
            
            successful_extractions = 0
            failed_extractions = 0
            
            for idx, img in enumerate(image_list):
                try:
                    xref = img[0]
                    
                    # Enhanced error handling for image extraction
                    try:
                        base_image = fitz_doc.extract_image(xref)
                        if not base_image:
                            print(f"    ‚ö†Ô∏è Image {idx + 1}: No image data returned")
                            failed_extractions += 1
                            continue
                            
                        img_bytes = base_image.get("image")
                        if not img_bytes:
                            print(f"    ‚ö†Ô∏è Image {idx + 1}: Empty image bytes")
                            failed_extractions += 1
                            continue
                            
                    except Exception as extract_error:
                        print(f"    ‚ö†Ô∏è Image {idx + 1}: Extraction failed - {extract_error}")
                        failed_extractions += 1
                        continue
                    
                    # Get image properties with fallbacks
                    img_ext = base_image.get("ext", "png")
                    width = base_image.get("width", 0)
                    height = base_image.get("height", 0)
                    
                    if width == 0 or height == 0:
                        print(f"    ‚ö†Ô∏è Image {idx + 1}: Invalid dimensions {width}x{height}")
                        failed_extractions += 1
                        continue
                    
                    # Get image position with enhanced error handling
                    try:
                        img_rect = pymupdf_page.get_image_bbox(img)
                        x0, y0, x1, y1 = img_rect
                        
                        # Validate coordinates
                        if x1 <= x0 or y1 <= y0:
                            print(f"    ‚ö†Ô∏è Image {idx + 1}: Invalid bounding box {x0},{y0},{x1},{y1}")
                            # Use fallback position
                            x0, y0 = 50 + (idx * 10), 50 + (idx * 10)
                            x1, y1 = x0 + width, y0 + height
                            
                    except Exception as coord_error:
                        print(f"    ‚ö†Ô∏è Image {idx + 1}: Coordinate extraction failed - {coord_error}")
                        # Use fallback position
                        x0, y0 = 50 + (idx * 10), 50 + (idx * 10)
                        x1, y1 = x0 + width, y0 + height
                    
                    # Convert coordinates to standard system
                    fixed_y0 = coord_converter.pymupdf_to_standard(y0, height)
                    fixed_y1 = coord_converter.pymupdf_to_standard(y1, height)
                    
                    image_info = {
                        "image_number": idx + 1,
                        "position": {
                            "x0": float(x0), 
                            "y0": float(fixed_y0),
                            "x1": float(x1),
                            "y1": float(fixed_y1),
                            "width": float(width), 
                            "height": float(height)
                        },
                        "dimensions": {"width": float(width), "height": float(height)},
                        "extraction_method": "enhanced_pymupdf",
                        "page": page_num,
                        "image_format": img_ext,
                        "image_size_bytes": len(img_bytes)
                    }
                    
                    # Enhanced bar chart detection
                    try:
                        page_text_blocks = self._extract_text_enhanced(page, coord_converter)
                        chart_detection = self.bar_chart_detector.detect_bar_chart_enhanced(
                            image_info, page_text_blocks, page_num
                        )
                        
                        axis_data = {}
                        if chart_detection['is_bar_chart_candidate']:
                            axis_data = extract_axis_labels_from_context(page_text_blocks, image_info['position'])
                        
                        image_info.update({
                            "is_bar_chart_candidate": chart_detection['is_bar_chart_candidate'],
                            "bar_chart_confidence": chart_detection['confidence'],
                            "chart_type": chart_detection['chart_type'],
                            "detection_features": chart_detection['detection_features'],
                            "extracted_axis_data": axis_data,
                            "description": f"Image {idx+1} ({width}x{height})" + 
                                          (" [BAR CHART]" if chart_detection['is_bar_chart_candidate'] else "")
                        })
                    except Exception as chart_error:
                        print(f"    ‚ö†Ô∏è Chart detection failed for image {idx + 1}: {chart_error}")
                        image_info.update({
                            "is_bar_chart_candidate": False,
                            "bar_chart_confidence": 0.0,
                            "chart_type": "unknown",
                            "detection_features": ["chart_detection_failed"]
                        })
                    
                    # Handle image data based on user preference
                    try:
                        if image_handling == "base64":
                            b64 = base64.b64encode(img_bytes).decode("utf-8")
                            image_info["data"] = b64
                            image_info["image_size_kb"] = len(img_bytes) / 1024
                        elif image_handling == "external":
                            img_filename = f"page_{page_num+1}_img_{idx+1}.{img_ext}"
                            img_path = Path("extracted_images") / img_filename
                            img_path.parent.mkdir(exist_ok=True)
                            with open(img_path, "wb") as f:
                                f.write(img_bytes)
                            image_info["external_path"] = str(img_path)
                            image_info["image_size_kb"] = len(img_bytes) / 1024
                        else:  # metadata only
                            image_info["image_size_kb"] = len(img_bytes) / 1024
                            
                    except Exception as data_error:
                        print(f"    ‚ö†Ô∏è Image data handling failed for image {idx + 1}: {data_error}")
                        # Continue with metadata only
                        image_info["image_size_kb"] = len(img_bytes) / 1024
                        image_info["data_handling_error"] = str(data_error)
                    
                    images.append(image_info)
                    successful_extractions += 1
                    
                    if image_info.get('is_bar_chart_candidate', False):
                        print(f"    ‚úÖ ENHANCED BAR CHART: Image {idx + 1} - Confidence: {image_info['bar_chart_confidence']:.2f}")
                    else:
                        print(f"    ‚úÖ Extracted image {idx + 1} ({width}x{height}) at position {x0:.1f}, {y0:.1f}")
                            
                except Exception as e:
                    print(f"    ‚ö†Ô∏è Enhanced image extraction failed for image {idx + 1}: {e}")
                    failed_extractions += 1
                    
            # Summary for the page
            if successful_extractions > 0:
                print(f"    üìä Image extraction summary: {successful_extractions} successful, {failed_extractions} failed")
            else:
                print(f"    ‚ùå No images successfully extracted from page {page_num + 1}")
                        
        except Exception as e:
            print(f"    ‚ùå Enhanced image extraction failed on page {page_num + 1}: {e}")
            
        return images

    def _extract_layout_enhanced(self, page, coord_converter):
        """Extract layout elements with consistent coordinates"""
        elements = []
        
        try:
            # Extract lines
            for line in page.lines:
                fixed_y0 = coord_converter.pdfplumber_to_standard(line['top'])
                fixed_y1 = coord_converter.pdfplumber_to_standard(line['bottom'])
                
                elements.append({
                    "type": "line",
                    "position": {
                        "x0": float(line['x0']), "y0": float(fixed_y0),
                        "x1": float(line['x1']), "y1": float(fixed_y1)
                    },
                    "width": float(line['width']), 
                    "height": float(line['height']),
                    "extraction_method": "enhanced"
                })
                
        except Exception as e:
            print(f"    ‚ö†Ô∏è Enhanced layout extraction failed: {e}")
            
        return elements

    def _extract_vector_symbols_enhanced(self, fitz_doc, page_num, coord_converter):
        """Extract vector symbols with consistent coordinates"""
        symbols = []
        
        try:
            page = fitz_doc[page_num]
            page_height = page.rect.height
            drawings = page.get_drawings()
            
            for d in drawings:
                bbox = d.get("rect", None)
                if not bbox:
                    continue
                    
                width = abs(bbox[2] - bbox[0])
                height = abs(bbox[3] - bbox[1])
                aspect = width / max(height, 1)
                
                # Convert coordinates
                fixed_y0 = coord_converter.pymupdf_to_standard(bbox[3], 0)
                fixed_y1 = coord_converter.pymupdf_to_standard(bbox[1], 0)
                
                # Enhanced symbol detection
                symbol_type = "unknown"
                if height < 3 and width > 10:
                    symbol_type = "fraction_bar"
                elif aspect < 0.2 and height > 10:
                    symbol_type = "vertical_line"
                elif 0.8 <= aspect <= 1.2 and width > 5 and height > 5:
                    symbol_type = "square_or_circle"
                
                symbols.append({
                    "type": "vector_symbol",
                    "symbol_guess": symbol_type,
                    "bbox": [bbox[0], fixed_y0, bbox[2], fixed_y1],
                    "dimensions": {"width": width, "height": height},
                    "source": "drawing-stroke",
                    "page": page_num,
                    "extraction_method": "enhanced"
                })
                
        except Exception as e:
            print(f"    ‚≠ê‚≠ê Enhanced vector symbol extraction failed: {e}")
            
        return symbols


# ============================================================
#  FIXED MATH CONTENT DETECTION - ADDED AT THE END
# ============================================================
def detect_math_content(text: str) -> bool:
    """
    FINAL FIX: Only classify as mathematical when content is PURE math
    - If content has () or ? symbols WITH normal text ‚Üí NOT mathematical
    - Only classify as mathematical if it's PURE numbers/math symbols
    """
    if not text or len(text.strip()) < 2:
        return False
    
    clean_text = text.strip()
    
    # Count PURE math symbols (exclude parentheses and question marks)
    pure_math_symbols = len(re.findall(r'[√∑√ó=‚àö¬≤¬≥‚àõ‚àú\+\-\*\/\%]', clean_text))
    numbers = len(re.findall(r'\b\d+\b', clean_text))
    
    # Count meaningful text words (at least 3 letters)
    normal_words = len(re.findall(r'\b[a-zA-Z]{3,}\b', clean_text))
    
    # ‚≠ê‚≠ê KEY FIX: If content has parentheses OR question marks WITH normal text ‚Üí NOT mathematical
    has_parentheses = '(' in clean_text or ')' in clean_text
    has_question_mark = '?' in clean_text
    
    # If it has parentheses/question marks AND normal text ‚Üí it's normal text/question
    if (has_parentheses or has_question_mark) and normal_words >= 2:
        return False
    
    # If it has substantial normal text, it's NOT mathematical
    if normal_words >= 3:
        return False
    
    # Calculate math ratio
    total_elements = pure_math_symbols + numbers + normal_words
    if total_elements == 0:
        return False
    
    math_ratio = (pure_math_symbols + numbers) / total_elements
    
    # Only classify as mathematical if overwhelmingly math-heavy
    return math_ratio >= 0.85

def update_content_types_to_mathematical(json_path: str, output_path: str = None):
    """
    Update content_type to 'mathematical' ONLY for PURE math content
    Fix incorrectly classified content with () and ? symbols
    """
    if output_path is None:
        output_path = json_path
    
    print(f"üî¢ Smart math content detection...")
    
    with open(json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    updated_count = 0
    fixed_count = 0
    
    for page in data.get('pages', []):
        for text_block in page.get('text_content', []):
            content = text_block.get('content', '')
            current_type = text_block.get('content_type')
            
            # Check if this is ACTUALLY mathematical content
            is_actually_math = detect_math_content(content)
            
            # Fix: If currently mathematical but shouldn't be
            if current_type == 'mathematical' and not is_actually_math:
                # Determine correct content type
                if re.match(r'^[1-5]\)', content.strip()):  # Options like "1) 25", "2) 30"
                    text_block['content_type'] = 'option'
                elif '?' in content and ('what' in content.lower() or 'which' in content.lower() or content.strip().endswith('?')):
                    text_block['content_type'] = 'question'
                elif 'direction' in content.lower() or 'instruction' in content.lower():
                    text_block['content_type'] = 'instruction'
                elif re.match(r'^\d+\.', content.strip()):  # Questions like "1. What is..."
                    text_block['content_type'] = 'question'
                else:
                    text_block['content_type'] = 'normal_text'
                fixed_count += 1
                print(f"   üîÑ Fixed: '{content[:50]}...' ‚Üí {text_block['content_type']}")
            
            # Update: If not mathematical but should be
            elif current_type != 'mathematical' and is_actually_math:
                text_block['content_type'] = 'mathematical'
                updated_count += 1
                print(f"   ‚úÖ Math: '{content[:50]}...'")
    
    # Save the modified data
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=2, ensure_ascii=False)
    
    print(f"‚úÖ Fixed {fixed_count} incorrect mathematical classifications")
    print(f"‚úÖ Updated {updated_count} blocks to mathematical content_type")
    
    return data


# ============================================================
#  MAIN EXECUTION (LEGACY EXTRACTION CLI)
# ============================================================
def extraction_cli_main():
    converter = PDFToJSONConverter()
    pdf_path = "SBI Clerk Prelims.pdf"
    
    print("üéØ FIXED PDF Extraction Modes:")
    print("1. Text + Tables only (fast)")
    print("2. Text + Tables + Image metadata (recommended)")
    print("3. Text + Tables + External images (best for analysis)")
    print("4. Everything in JSON (large files)")
    
    choice = input("Choose mode (1-4, default 2): ").strip() or "2"
    
    modes = {
        "1": {"include_images": False, "image_handling": "metadata"},
        "2": {"include_images": True, "image_handling": "metadata"},
        "3": {"include_images": True, "image_handling": "external"},
        "4": {"include_images": True, "image_handling": "base64"}
    }
    
    config = modes.get(choice, modes["2"])
    output_json = "sbi_extracted.json"
    
    # Run enhanced extraction
    try:
        result = converter.convert_pdf_to_json_enhanced(
            pdf_path, 
            output_json, 
            **config
        )
        
        print(f"\nüéâ FIXED extraction completed!")
        print(f"üìä Validation Score: {result['validation']['summary']['extraction_completeness']:.2f}")
        print(f"üìÑ Total pages: {result['validation']['summary']['total_pages']}")
        print(f"üìù Text blocks: {result['validation']['summary']['total_text_blocks']}")
        print(f"üî¢ Math/numeric blocks: {result['validation']['summary']['total_math_blocks']}")
        print(f"üìä Tables: {result['validation']['summary']['total_tables']}")
        print(f"üñºÔ∏è Images: {result['validation']['summary']['total_images']}")
        
        # Apply the mathematical to normal_text conversion
        print(f"\nüîÑ Applying mathematical to normal_text conversion...")
        change_mathematical_to_normal_text(output_json, output_json)
        
        # Apply math content detection - NEW LINE ADDED
        print(f"\nüî¢ Detecting mathematical content...")
        update_content_types_to_mathematical(output_json, output_json)
        
    except Exception as e:
        print(f"‚ùå Enhanced extraction failed: {e}")


# ======================================================================
# JSON Translator
# ======================================================================
"""
Professional Interactive Translator for JSON Documents
------------------------------------------------------
‚Üí Lets you select Indian languages (Hindi, Telugu, Odia, etc.)
‚Üí Shows live translation in console  
‚Üí Saves layout-faithful translated JSON
‚Üí VERIFIES vector symbols and layout lines are preserved
‚Üí PRESERVES COORDINATES during translation
‚Üí SMART CONTENT FILTERING for better translation quality
‚Üí SUPPORTS SEPARATED MATH/NUMERIC CONTENT
"""


# ============================================================
# CONFIGURATION
# ============================================================
INPUT_JSON = "sbi_extracted.json"  # your extracted JSON file
OUTPUT_DIR = "translated_jsons"
MAX_RETRIES = 3
RETRY_DELAY = 2

# ============================================================
# LANGUAGE OPTIONS  
# ============================================================
LANG_OPTIONS = {
    "1": ("hi", "Hindi"),
    "2": ("te", "Telugu"),
    "3": ("or", "Odia"),
    "4": ("ta", "Tamil"),
    "5": ("ml", "Malayalam"),
    "6": ("bn", "Bengali"),
    "7": ("gu", "Gujarati"),
    "8": ("pa", "Punjabi"),
    "9": ("mr", "Marathi")
}

LANG_CODE_TO_NAME = {code: name for code, name in LANG_OPTIONS.values()}
LANG_NAME_TO_CODE = {name.lower(): code for code, name in LANG_OPTIONS.values()}


def _extract_bbox(entry: Dict[str, Any]) -> List[float] | None:
    """Normalize bbox/position dictionaries for coordinate comparisons."""
    if not isinstance(entry, dict):
        return None
    bbox = entry.get("bbox")
    if bbox and len(bbox) == 4:
        return [float(v) for v in bbox]
    position = entry.get("position") or {}
    if position:
        x0 = float(position.get("x0", 0.0))
        y0 = float(position.get("y0", 0.0))
        x1 = float(position.get("x1", x0 + position.get("width", 0.0)))
        y1 = float(position.get("y1", y0 + position.get("height", 0.0)))
        return [x0, y0, x1, y1]
    return None

# ============================================================
# ENHANCED CONTENT FILTERING - CRITICAL FOR GOVERNMENT EXAMS
# ============================================================
def should_translate_content(text: str, content_type: str = None) -> bool:
    """
    Smart content filtering for government exam papers
    Returns False for content that should NOT be translated
    """
    if not text or not text.strip():
        return False
    
    text_clean = text.strip()
    
    # ENHANCED: Check if content_type might be incorrectly set
    # If text is clearly descriptive English with percentages, override mathematical type
    if content_type == 'mathematical':
        # Check if this is actually descriptive text with percentages
        words = re.findall(r'\b[a-zA-Z]+\b', text_clean)
        if len(words) >= 3:  # If there are at least 3 words, it's likely descriptive text
            # Count percentage symbols
            percent_count = text_clean.count('%')
            # If it's mostly text with some percentages, override the content_type
            if percent_count <= 2 and len(words) > percent_count * 2:
                content_type = 'normal_text'  # Override incorrectly set content_type
    
    # Use content_type from JSON if available (highest priority)
    if content_type and content_type in ['mathematical', 'data_label', 'option', 'coordinate']:
        return False
    
    # Don't translate pure numbers/options (like "1) 445") but allow mixed content
    if re.match(r'^[1-5]\)\s*\d+$', text_clean):
        return False
    
    # Don't translate table headers that are purely numbers
    if re.match(r'^\d+\s+\d+\s+\d+\s+\d+\s+\d+\s+\?$', text_clean):
        return False
    
    # Don't translate pure English acronyms that should stay same
    acronyms = ['SBI', 'LIC', 'NIACL', 'MTS', 'CGL', 'CHSL', 'CCE', 'SSC', 'UPSC', 'IBPS']
    if all(word in acronyms or word.isdigit() for word in text_clean.upper().split()):
        return False
    
    # Don't translate single characters UNLESS they are words
    if len(text_clean) <= 1:
        return False
    elif len(text_clean) == 2:
        # Allow common 2-letter words but block pure numbers/symbols
        if text_clean.lower() in ['is', 'am', 'are', 'to', 'of', 'in', 'on', 'at', 'by', 'we', 'us', 'he', 'she', 'it', 'my', 'your', 'our', 'their', 'an', 'as', 'or', 'if', 'so', 'no', 'yes', 'ok']:
            return True
        elif text_clean.isalpha():
            return True
        else:
            return False
    
    # Don't translate pure data labels (years, numbers) but allow them in sentences
    if re.match(r'^(201[2-9]|202[0-9]|60|50|40|30|20|10|0)$', text_clean) and len(text_clean) <= 4:
        return False
    
    # ENHANCED MATH DETECTION: Only block PURE mathematical expressions
    # Keep % symbol since it's common in text descriptions
    math_indicators = ['‚àö', '√ó', '√∑', '¬≤', '¬≥', '‚àõ', '‚àú', '=']  # Removed '%' and '^' as they appear in text
    
    # Check for complex mathematical expressions (multiple math symbols)
    math_symbols_found = [symbol for symbol in math_indicators if symbol in text_clean]
    
    if math_symbols_found:
        # Count words and math symbols more accurately
        words = re.findall(r'\b[a-zA-Z]+\b', text_clean)  # Only count actual words
        math_symbol_count = len(math_symbols_found)
        
        # If no meaningful words found and multiple math symbols, it's likely pure math
        if len(words) == 0 and math_symbol_count >= 2:
            return False
        
        # Only block if it's primarily math (more than 60% math content)
        if len(words) > 0 and math_symbol_count > max(len(words) * 0.6, 3):
            return False
        # Allow mixed content (text with some math symbols)
        else:
            return True
    
    # ENHANCED: Allow percentage text and other common patterns
    # Check if text contains percentage but is actually descriptive text
    if '%' in text_clean:
        # Count words in the text to determine if it's descriptive
        words = re.findall(r'\b[a-zA-Z]+\b', text_clean)
        if len(words) >= 3:  # If there are at least 3 words, it's likely descriptive text
            return True
    
    # FIXED: IMPROVED TEXT+NUMBER DETECTION
    # Check if this is descriptive text with numbers (should be translated)
    words = re.findall(r'\b[a-zA-Z]+\b', text_clean)
    numbers = re.findall(r'\b\d+\b', text_clean)
    
    # If it has both words AND numbers, and at least 2 words, it's descriptive text
    if len(words) >= 2 and len(numbers) >= 1:
        return True
    
    # If it's mostly text with some symbols, translate it
    if len(words) >= 3:
        return True
    
    return True

def get_content_category(text: str) -> str:
    """Categorize content for better translation handling"""
    if not text:
        return "unknown"
    
    text_lower = text.lower().strip()
    
    # Instructions
    if any(word in text_lower for word in ['directions', 'study the', 'answer the', 'following questions', 'read the']):
        return "instruction"
    
    # Questions (starts with number and period)
    elif re.match(r'^\d+\.', text.strip()):
        return "question"
    
    # Options (starts with number and parenthesis)
    elif re.match(r'^[1-5]\)', text.strip()):
        return "option"
    
    # Data labels (years, numbers on axes)
    elif re.match(r'^(201[2-9]|202[0-9]|60|50|40|30|20|10|0)$', text.strip()):
        return "data_label"
    
    # Mathematical content
    elif any(symbol in text for symbol in ['√∑', '√ó', '=', '%', '‚àö', '¬≤', '¬≥']):
        return "mathematical"
    
    # Headers
    elif re.match(r'^[A-Z][A-Z\s]+$', text.strip()):
        return "header"
    
    else:
        return "normal_text"

# ============================================================
# HELPERS - ENHANCED WITH COORDINATE PRESERVATION
# ============================================================
def safe_translate(text: str, target_lang: str) -> str:
    """Translate safely with retry and log in console."""
    if not text.strip():
        return text
    
    # Enhanced text cleaning before translation
    text = text.strip()
    
    for attempt in range(1, MAX_RETRIES + 1):
        try:
            translated = GoogleTranslator(source="auto", target=target_lang).translate(text)
            print(f"   ‚úÖ Translation successful (attempt {attempt})")
            return translated
        except Exception as e:
            print(f"‚ö†Ô∏è Retry {attempt}/{MAX_RETRIES} failed: {e}")
            if attempt < MAX_RETRIES:
                time.sleep(RETRY_DELAY)
    
    print(f"‚ùå All translation attempts failed for: {text[:50]}...")
    return text  # Return original if all attempts fail

def count_json_elements(data):
    """Count all critical elements in JSON for verification"""
    vector_count = 0
    layout_count = 0
    image_count = 0
    table_count = 0
    text_blocks = 0
    math_blocks = 0
    translatable_blocks = 0
    
    for page in data.get("pages", []):
        vector_count += len(page.get('vector_symbols', []))
        layout_count += len(page.get('layout_elements', []))
        image_count += len(page.get('images', []))
        table_count += len(page.get('tables', []))
        math_blocks += len(page.get('math_numeric_content', []))
        
        for text_block in page.get('text_content', []):
            text_blocks += 1
            content = text_block.get('content', '')
            content_type = text_block.get('content_type', '')
            if should_translate_content(content, content_type):
                translatable_blocks += 1
    
    return {
        'vector_symbols': vector_count,
        'layout_elements': layout_count,
        'images': image_count,
        'tables': table_count,
        'text_blocks': text_blocks,
        'math_blocks': math_blocks,
        'translatable_blocks': translatable_blocks
    }

def verify_coordinate_preservation(original_data, translated_data):
    """Verify that coordinates are preserved during translation"""
    print(f"\nüéØ COORDINATE PRESERVATION CHECK:")
    
    original_pages = original_data.get("pages", [])
    translated_pages = translated_data.get("pages", [])
    
    if len(original_pages) != len(translated_pages):
        print("‚ùå Page count mismatch!")
        return False
    
    all_coordinates_preserved = True
    
    def compare_bbox_sequences(orig_seq, trans_seq, label, page_idx):
        nonlocal all_coordinates_preserved
        if len(orig_seq) != len(trans_seq):
            print(f"‚ùå Page {page_idx + 1}: {label} count mismatch ({len(orig_seq)} vs {len(trans_seq)})")
            all_coordinates_preserved = False
            return
        for idx, (orig_item, trans_item) in enumerate(zip(orig_seq, trans_seq), start=1):
            orig_bbox = _extract_bbox(orig_item) or []
            trans_bbox = _extract_bbox(trans_item) or []
            if orig_bbox != trans_bbox:
                print(f"‚ùå Page {page_idx + 1}, {label} {idx}: coordinates changed!")
                print(f"   Original: {orig_bbox}")
                print(f"   Translated: {trans_bbox}")
                all_coordinates_preserved = False

    for page_idx, (orig_page, trans_page) in enumerate(zip(original_pages, translated_pages)):
        compare_bbox_sequences(orig_page.get("vector_symbols", []),
                               trans_page.get("vector_symbols", []),
                               "Vector symbol",
                               page_idx)
        compare_bbox_sequences(orig_page.get("layout_elements", []),
                               trans_page.get("layout_elements", []),
                               "Layout element",
                               page_idx)
        compare_bbox_sequences(orig_page.get("text_content", []),
                               trans_page.get("text_content", []),
                               "Text block",
                               page_idx)
        compare_bbox_sequences(orig_page.get("math_numeric_content", []),
                               trans_page.get("math_numeric_content", []),
                               "Math block",
                               page_idx)
    
    if all_coordinates_preserved:
        print("‚úÖ SUCCESS: All coordinates preserved during translation!")
    else:
        print("‚ùå WARNING: Some coordinates were modified during translation!")
    
    return all_coordinates_preserved

def check_text_number_combination(json_data):
    """Check how text+number combinations are handled"""
    print("\nüîç CHECKING TEXT + NUMBER COMBINATIONS")
    print("=" * 50)
    
    text_with_numbers = 0
    pure_text = 0
    pure_numbers = 0
    mixed_content_blocks = []
    
    for page_num, page in enumerate(json_data.get('pages', [])):
        for block in page.get('text_content', []):
            content = block.get('content', '')
            
            # Count words and numbers
            words = re.findall(r'\b[a-zA-Z]+\b', content)
            numbers = re.findall(r'\b\d+\b', content)
            
            if words and numbers:
                text_with_numbers += 1
                if text_with_numbers <= 5:  # Show first 5 examples
                    mixed_content_blocks.append(content)
                    print(f"   üìù Text+Numbers [{len(words)}w, {len(numbers)}n]: '{content[:80]}...'")
            elif words:
                pure_text += 1
            elif numbers:
                pure_numbers += 1
    
    print(f"\nüìä CLASSIFICATION SUMMARY:")
    print(f"   Text + Numbers blocks: {text_with_numbers}")
    print(f"   Pure Text blocks: {pure_text}") 
    print(f"   Pure Numbers blocks: {pure_numbers}")
    
    return text_with_numbers, mixed_content_blocks

def check_translated_json_coordinates(original_json: str | None = None,
                                      translated_json: str | None = None):
    """Check if existing translated JSON preserved coordinates"""
    print("\nüîç CHECKING EXISTING TRANSLATED JSON COORDINATES")
    print("=" * 50)
    
    if original_json is None:
        original_json = INPUT_JSON
    if translated_json is None:
        translated_json = str(Path(OUTPUT_DIR) / f"{Path(INPUT_JSON).stem}_te.json")
    
    if not os.path.exists(original_json) or not os.path.exists(translated_json):
        print("‚ùå One or both JSON files not found for coordinate check")
        return False
    
    try:
        with open(original_json, 'r', encoding='utf-8') as f:
            original_data = json.load(f)
        with open(translated_json, 'r', encoding='utf-8') as f:
            translated_data = json.load(f)
        return verify_coordinate_preservation(original_data, translated_data)
    except Exception as e:
        print(f"‚ùå Error checking translated JSON: {e}")
        return False

def translate_json_file(input_json: str,
                        target_lang: str,
                        lang_name: str,
                        output_dir: str = OUTPUT_DIR):
    """Translate JSON file into given language while preserving ALL elements including coordinates."""    
    # Check if input file exists
    if not os.path.exists(input_json):
        print(f"‚ùå Error: Input file '{input_json}' not found!")
        return None
    
    # Load original data FIRST
    with open(input_json, "r", encoding="utf-8") as f:
        original_data = json.load(f)

    # ‚≠ê‚≠ê NEW: Check text+number combinations before translation
    text_with_numbers_count, mixed_examples = check_text_number_combination(original_data)
    print(f"\nüîç FOUND {text_with_numbers_count} TEXT+NUMBER BLOCKS THAT WILL BE TRANSLATED")

    # ‚≠ê‚≠ê DEEP COPY TO PRESERVE ORIGINAL STRUCTURE ‚≠ê‚≠ê
    data = copy.deepcopy(original_data)  # This preserves ALL original data including coordinates
    
    # üñºÔ∏è FIX: Preserve all image data from the original JSON
    preserved_images = 0
    for page_index, (orig_page, trans_page) in enumerate(zip(original_data.get("pages", []), data.get("pages", []))):
        orig_images = orig_page.get("images", [])
        trans_images = trans_page.get("images", [])
        for img_index, (orig_img, trans_img) in enumerate(zip(orig_images, trans_images)):
            if not trans_img.get("data") and orig_img.get("data"):
                trans_img["data"] = orig_img["data"]
                preserved_images += 1
                print(f"   ‚úÖ Preserved image {img_index+1} on page {page_index+1}")
    if preserved_images > 0:
        print(f"\nüñºÔ∏è TOTAL IMAGES PRESERVED FROM ORIGINAL JSON: {preserved_images}\n")
    else:
        print("\n‚ö†Ô∏è No images required preservation (all already intact)\n")

    # ---- Continue with original logic ----
    original_counts = count_json_elements(original_data)
    print(f"üîç PRE-TRANSLATION ANALYSIS:")
    print(f"   ‚Ä¢ Vector symbols: {original_counts['vector_symbols']}")
    print(f"   ‚Ä¢ Layout lines: {original_counts['layout_elements']}")
    print(f"   ‚Ä¢ Images: {original_counts['images']}")
    print(f"   ‚Ä¢ Tables: {original_counts['tables']}")
    print(f"   ‚Ä¢ Math/numeric blocks: {original_counts['math_blocks']}")
    print(f"   ‚Ä¢ Total text blocks: {original_counts['text_blocks']}")
    print(f"   ‚Ä¢ Translatable blocks: {original_counts['translatable_blocks']}")

    total_translated = 0
    total_skipped = 0
    output_dir_path = Path(output_dir)
    output_dir_path.mkdir(exist_ok=True)
    print(f"\nüåê TRANSLATING ‚Üí {lang_name} ({target_lang})")
    print("=" * 80)

    # ‚≠ê‚≠ê SMART TRANSLATION WITH CONTENT FILTERING ‚≠ê‚≠ê
    for p, page in enumerate(data.get("pages", []), start=1):
        page_translated = 0
        page_skipped = 0
        
        print(f"\nüìÑ Page {p}:")
        
        # ‚≠ê‚≠ê NEW: Process math_numeric_content blocks (preserve them without translation)
        math_blocks = page.get("math_numeric_content", [])
        if math_blocks:
            print(f"   üî¢ Math/Numeric blocks preserved: {len(math_blocks)}")
            for math_block in math_blocks:
                # Mark math blocks as non-translatable
                math_block["translation_ready"] = False
                math_block["content_type"] = "math_numeric"
        
        # ‚≠ê‚≠ê Process text_content blocks (translate only appropriate content)
        for block_idx, block in enumerate(page.get("text_content", []), start=1):
            original_text = block.get("content", "").strip()
            content_type = block.get("content_type", "")
            
            if not original_text:
                continue

            # ‚≠ê‚≠ê SMART CONTENT FILTERING ‚≠ê‚≠ê
            if not should_translate_content(original_text, content_type):
                print(f"   ‚è≠Ô∏è  SKIPPED [{block_idx}]: {original_text[:60]}...")
                page_skipped += 1
                total_skipped += 1
                continue

            # ‚≠ê‚≠ê TRANSLATE ONLY APPROPRIATE CONTENT ‚≠ê‚≠ê
            print(f"   üîÑ TRANSLATING [{block_idx}]: {original_text[:60]}...")            
            translated_text = safe_translate(original_text, target_lang)
            
            # ‚≠ê‚≠ê CRITICAL: Only add translated_content field, preserve all existing fields ‚≠ê‚≠ê
            block["translated_content"] = translated_text
            block["translation_metadata"] = {
                "original_language": "en",
                "target_language": target_lang,
                "content_type": content_type,
                "translated_at": time.strftime("%Y-%m-%d %H:%M:%S")
            }
            
            page_translated += 1
            total_translated += 1

            # Show side-by-side preview
            src_preview = original_text[:80] + "..." if len(original_text) > 80 else original_text
            trans_preview = translated_text[:80] + "..." if len(translated_text) > 80 else translated_text
            
            print(f"     üü¶ English : {src_preview}")
            print(f"     üü© {lang_name}: {trans_preview}")
            print("     " + "-" * 70)

        print(f"   üìä Page {p} summary: {page_translated} translated, {page_skipped} skipped, {len(math_blocks)} math blocks preserved")

    # ‚≠ê‚≠ê POST-TRANSLATION VERIFICATION ‚≠ê‚≠ê
    final_counts = count_json_elements(data)
    
    print(f"\n‚úÖ POST-TRANSLATION VERIFICATION:")
    print(f"   ‚Ä¢ Vector symbols: {final_counts['vector_symbols']} (preserved: {final_counts['vector_symbols'] == original_counts['vector_symbols']})")
    print(f"   ‚Ä¢ Layout lines: {final_counts['layout_elements']} (preserved: {final_counts['layout_elements'] == original_counts['layout_elements']})")
    print(f"   ‚Ä¢ Images: {final_counts['images']} (preserved: {final_counts['images'] == original_counts['images']})")
    print(f"   ‚Ä¢ Tables: {final_counts['tables']} (preserved: {final_counts['tables'] == original_counts['tables']})")
    print(f"   ‚Ä¢ Math/numeric blocks: {final_counts['math_blocks']} (preserved: {final_counts['math_blocks'] == original_counts['math_blocks']})")
    print(f"   ‚Ä¢ Translation stats: {total_translated} translated, {total_skipped} skipped")
    
    # ‚≠ê‚≠ê COORDINATE PRESERVATION CHECK ‚≠ê‚≠ê
    coordinate_check = verify_coordinate_preservation(original_data, data)
    
    # Check if all structural elements are preserved
    all_preserved = (
        final_counts['vector_symbols'] == original_counts['vector_symbols'] and
        final_counts['layout_elements'] == original_counts['layout_elements'] and
        final_counts['images'] == original_counts['images'] and
        final_counts['tables'] == original_counts['tables'] and
        final_counts['math_blocks'] == original_counts['math_blocks'] and
        coordinate_check
    )
    
    if all_preserved:
        print("üéâ SUCCESS: All structural elements AND coordinates preserved perfectly!")
    else:
        print("‚ùå WARNING: Some elements may have been modified!")

    # Save file with absolute path
    output_path = output_dir_path / f"{Path(input_json).stem}_{target_lang}.json"
    
    try:
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"\n‚úÖ SUCCESSFULLY SAVED TRANSLATED FILE:")
        print(f"   üìç Location: {output_path.absolute()}")
        print(f"   üìä File size: {output_path.stat().st_size} bytes")
        print(f"   üî§ Translation stats: {total_translated} translated, {total_skipped} skipped")
        print(f"   üèóÔ∏è  Structural elements preserved: {all_preserved}")
        print(f"   üìê Coordinates preserved: {coordinate_check}")
        
        return output_path
        
    except Exception as e:
        print(f"‚ùå Error saving file: {e}")
        return None

# ============================================================
# MAIN DRIVER - ENHANCED WITH SMART FILTERING (LEGACY CLI)
# ============================================================
def translator_cli_main():
    print("\n" + "="*60)
    print("üåê PROFESSIONAL JSON TRANSLATOR WITH SMART CONTENT FILTERING")
    print("="*60)
    print(f"üìÇ Current working directory: {os.getcwd()}")
    print(f"üìÑ Input file: {INPUT_JSON}")
    print(f"üìÅ Output directory: {OUTPUT_DIR}")
    
    # First check existing translated JSON if it exists
    existing_translation_check = check_translated_json_coordinates()
    
    # Check if input file exists
    if not os.path.exists(INPUT_JSON):
        print(f"\n‚ùå ERROR: Input file '{INPUT_JSON}' not found!")
        print("Please make sure:")
        print("1. The JSON file exists in the same directory as this script")
        print("2. The filename is spelled correctly")
        print(f"\nFiles in current directory:")
        for file in os.listdir('.'):
            if file.endswith('.json'):
                print(f"   üìÑ {file}")
            else:
                print(f"   üìÅ {file}")
        return

    print(f"\n‚úÖ Input file found: {INPUT_JSON}")
    print(f"\nüéØ SELECT TRANSLATION LANGUAGES:")
    for key, (code, name) in LANG_OPTIONS.items():
        print(f"  {key}. {name}")
    selection = input("\nEnter your choices (e.g. 1,2,3): ").strip()
    if not selection:
        print("‚ùå No languages selected. Exiting.")
        return

    choices = [s.strip() for s in selection.split(",") if s.strip() in LANG_OPTIONS]
    
    if not choices:
        print("‚ùå Invalid selection. Please choose valid numbers from the list.")
        return

    saved_files = []
    for choice in choices:
        code, name = LANG_OPTIONS[choice]
        print(f"\n{'='*80}")
        print(f"PROCESSING: {name} ({code})")
        print(f"{'='*80}")
        
        result = translate_json_file(INPUT_JSON, code, name, OUTPUT_DIR)
        if result:
            saved_files.append(result)
        
        # Small delay between languages to avoid API rate limits
        if len(choices) > 1:
            time.sleep(2)

    print("\n" + "="*80)
    print("üéâ TRANSLATION SUMMARY:")
    print(f"üìä Total languages processed: {len(choices)}")
    print(f"üíæ Files successfully saved: {len(saved_files)}")
    
    for file_path in saved_files:
        file_size = file_path.stat().st_size
        print(f"   ‚úÖ {file_path.name} ({file_size} bytes)")
    
    print(f"\nüìÅ All translated files are saved in: {Path(OUTPUT_DIR).absolute()}")
    
    # Final verification
    print(f"\nüîç FINAL VERIFICATION:")
    final_check = check_translated_json_coordinates()
    
    if final_check:
        print("üéâ PERFECT! Your translated JSON files are ready for PDF generation!")
    else:
        print("‚ö†Ô∏è  Some coordinate issues detected. Consider re-running translation.")
    
    # Final reminder
    print(f"\nüí° IMPORTANT FEATURES:")
    print(f"   ‚Ä¢ SMART FILTERING: Mathematical content, options, and data labels are preserved")
    print(f"   ‚Ä¢ SEPARATED MATH/NUMBERS: Math symbols and numbers are in separate blocks")
    print(f"   ‚Ä¢ COORDINATE PRESERVATION: All layout coordinates are maintained")
    print(f"   ‚Ä¢ STRUCTURAL INTEGRITY: Vector symbols, layout lines, images, math blocks preserved")
    print(f"   ‚Ä¢ TRANSLATION METADATA: Added translation tracking information")
    print(f"   ‚Ä¢ READY FOR PDF GENERATION: All elements preserved for accurate reconstruction!")

# ======================================================================
# PDF Creation
# ======================================================================
# -------------------- CONFIG --------------------
FONTS = {
    "telugu": r"E:\PDFExtraction\fonts\NotoSansTelugu-Regular.ttf",
    "hindi": r"E:\PDFExtraction\fonts\TiroDevanagariHindi-Regular.ttf",
    "odia": r"E:\PDFExtraction\fonts\AnekOdia-Regular.ttf"
}

# -------------------- UTILS --------------------
def clean(s: str) -> str:
    if not s:
        return ""
    s = html.unescape(str(s))
    return s.replace("&lt;", "<").replace("&gt;", ">").replace("&amp;", "&").strip()

def detect_language(data) -> str:
    sample = json.dumps(data, ensure_ascii=False)
    if "telugu" in sample.lower():
        return "telugu"
    if "hindi" in sample.lower():
        return "hindi"
    if "odia" in sample.lower() or "oriya" in sample.lower():
        return "odia"
    # fallback to unicode-range heuristic
    hindi = re.findall(r'[\u0900-\u097F]', sample)
    telugu = re.findall(r'[\u0C00-\u0C7F]', sample)
    odia = re.findall(r'[\u0B00-\u0B7F]', sample)
    if len(hindi) >= len(telugu) and len(hindi) >= len(odia):
        return "hindi"
    if len(telugu) >= len(hindi) and len(telugu) >= len(odia):
        return "telugu"
    if len(odia) >= len(hindi) and len(odia) >= len(telugu):
        return "odia"
    return "telugu"

def _page_height_from(page_data):
    dims = page_data.get("dimensions")
    if isinstance(dims, dict):
        return dims.get("height", 842)
    if isinstance(dims, (list, tuple)) and len(dims) >= 2:
        return dims[1]
    return 842

def _top_from_bottom(y_bottom, elem_height, page_height):
    return page_height - (y_bottom + elem_height)

# -------------------- HTML PAGE BUILDER (Playwright) --------------------
def build_page_html(page_data, lang):
    font_file = FONTS.get(lang, FONTS["telugu"])
    font_path = Path(font_file).resolve().as_uri() if os.path.exists(font_file) else ""

    css = f"""
    @font-face {{
        font-family: 'LangFont';
        src: url('{font_path}') format('truetype');
    }}
    *{{margin:0;padding:0;box-sizing:border-box}}
    body{{font-family:LangFont,sans-serif;width:595px;height:842px;background:#fff}}
    .text-element{{position:absolute;white-space:pre-wrap;background:transparent;color:#000;z-index:3}}
    .image-element{{position:absolute;z-index:2;border:none}}
    .line-element{{position:absolute;background:#000;transform-origin:0 0;z-index:3}}
    .rect-element{{position:absolute;border:1px solid #000;background:transparent;z-index:3}}
    """

    page_h = _page_height_from(page_data)
    html_parts = [
        "<!doctype html><html><head><meta charset='utf-8'>",
        f"<style>{css}</style></head><body>"
    ]

    # full white background (covers original visually)
    html_parts.append(f'<div style="position:absolute;left:0;top:0;width:595px;height:{page_h}px;background:#ffffff;z-index:0;"></div>')

    # images
    for image in page_data.get("images", []):
        pos = image.get("position", {})
        x0 = pos.get("x0", 0)
        width = pos.get("width", 0)
        height = pos.get("height", 0)
        y0 = _top_from_bottom(pos.get("y0", 0), height, page_h)
        img_b64 = image.get("data", "")
        img_fmt = image.get("image_format", "png").lower()
        if img_fmt == "jpg":
            img_fmt = "jpeg"
        if img_b64:
            try:
                img_src = f"data:image/{img_fmt};base64,{img_b64}"
                html_parts.append(f'<img class="image-element" src="{img_src}" style="left:{x0}px;top:{y0}px;width:{width}px;height:{height}px;">')
            except Exception:
                pass

    # text - ONLY ALLOWED CONTENT TYPES
    allowed_content_types = ["question", "exam_reference", "normal_text", "header", "instruction"]
    for tb in page_data.get("text_content", []):
        content_type = tb.get("content_type", "")
        if content_type not in allowed_content_types:
            continue
            
        content = tb.get("translated_content") or tb.get("content") or ""
        if not content:
            continue
        pos = tb.get("position", {})
        font_size = tb.get("font", {}).get("size", 12)
        x0 = pos.get("x0", 0)
        y0 = _top_from_bottom(pos.get("y0", 0), font_size, page_h)
        safe = clean(content)
        html_parts.append(f'<div class="text-element" style="left:{x0}px;top:{y0}px;font-size:{font_size}px;line-height:1.15;">{safe}</div>')

    # lines and rects
    for el in page_data.get("layout_elements", []):
        if el.get("type") == "line":
            x0 = el['position']['x0']; y0b = el['position']['y0']
            x1 = el['position']['x1']; y1b = el['position']['y1']
            y0 = _top_from_bottom(y0b, 0, page_h); y1 = _top_from_bottom(y1b, 0, page_h)
            width_len = math.hypot(x1 - x0, y1 - y0)
            angle = -math.atan2(y1 - y0, x1 - x0) if x1 != x0 else -math.pi/2
            html_parts.append(f'<div class="line-element" style="left:{x0}px;top:{y0}px;width:{width_len}px;height:1px;transform:rotate({angle}rad);"></div>')
    for sym in page_data.get("vector_symbols", []):
        x0, y0o, x1, y1o = sym['bbox']
        h = y1o - y0o
        y0 = _top_from_bottom(y0o, h, page_h)
        w = x1 - x0
        html_parts.append(f'<div class="rect-element" style="left:{x0}px;top:{y0}px;width:{w}px;height:{h}px;"></div>')

    html_parts.append("</body></html>")
    return "\n".join(html_parts)

# -------------------- PDF GENERATOR (Playwright + fallback) --------------------
class PDFGenerator:
    def __init__(self, json_data, output_pdf, original_pdf_path=None):
        if isinstance(json_data, str):
            with open(json_data, 'r', encoding='utf-8') as f:
                self.data = json.load(f)
        else:
            self.data = json_data
        self.output_pdf = output_pdf
        self.original_pdf_path = original_pdf_path
        self._register_fonts()

    def _register_fonts(self):
        try:
            for lang, font_path in FONTS.items():
                if os.path.exists(font_path):
                    pdfmetrics.registerFont(TTFont(lang.capitalize(), font_path))
        except Exception:
            pass

    def generate_pdf(self):
        pages = self.data.get("pages", [])
        if not pages:
            raise RuntimeError("No pages in JSON")

        lang = detect_language(self.data)
        temp_pdfs = []
        merger = PdfMerger()

        try:
            with sync_playwright() as p:
                browser = p.chromium.launch()
                for i, page_data in enumerate(pages):
                    html_content = build_page_html(page_data, lang)
                    with tempfile.NamedTemporaryFile(delete=False, suffix=".html", mode="w", encoding="utf-8") as fh:
                        fh.write(html_content)
                        html_path = fh.name
                    page = browser.new_page()
                    page.set_viewport_size({"width": 595, "height": int(_page_height_from(page_data))})
                    page.goto(f"file://{html_path}")
                    page.wait_for_timeout(700)
                    tmp_pdf = f"temp_page_{i}.pdf"
                    page.pdf(path=tmp_pdf, width=f"{595}px", height=f"{int(_page_height_from(page_data))}px", margin={"top":"0","right":"0","bottom":"0","left":"0"}, print_background=True)
                    merger.append(tmp_pdf)
                    temp_pdfs.append(tmp_pdf)
                    page.close()
                    os.unlink(html_path)
                browser.close()

                # if original provided, overlay generated pages onto original
                if self.original_pdf_path and os.path.exists(self.original_pdf_path):
                    reader = PdfReader(self.original_pdf_path)
                    writer = PdfWriter()
                    for idx, base_page in enumerate(reader.pages):
                        overlay_page = None
                        if idx < len(temp_pdfs):
                            overlay_reader = PdfReader(temp_pdfs[idx])
                            overlay_page = overlay_reader.pages[0]
                        if overlay_page:
                            base_page.merge_page(overlay_page)
                        writer.add_page(base_page)
                    with open(self.output_pdf, "wb") as out_f:
                        writer.write(out_f)
                else:
                    merger.write(self.output_pdf)
                    merger.close()

        except Exception as e:
            # fallback to ReportLab drawing (page-by-page)
            self.generate_pdf_fallback()
        finally:
            for tp in temp_pdfs:
                if os.path.exists(tp):
                    os.unlink(tp)

    def generate_pdf_fallback(self):
        pages = self.data.get("pages", [])
        pdf = canvas.Canvas(self.output_pdf, pagesize=(595, 842))
        for idx, pd in enumerate(pages):
            if idx > 0:
                pdf.showPage()
            # white background to cover originals
            pdf.setFillColor(colors.white)
            pdf.rect(0, 0, 595, 842, stroke=0, fill=1)
            self.draw_images(pdf, pd)
            self.draw_text(pdf, pd)
            self.draw_layout_elements(pdf, pd)
            self.draw_vector_symbols(pdf, pd)
        pdf.save()

    def draw_images(self, pdf, page_data):
        page_h = _page_height_from(page_data)
        for img in page_data.get("images", []):
            pos = img['position']
            x0 = pos['x0']
            h = pos['height']
            y0 = _top_from_bottom(pos['y0'], h, page_h)
            w = pos['width']
            data = img.get('data')
            if not data:
                continue
            try:
                img_bytes = base64.b64decode(data)
                pdf.drawImage(ImageReader(BytesIO(img_bytes)), x0, y0, w, h)
            except Exception:
                pass

    def draw_text(self, pdf, page_data):
        page_h = _page_height_from(page_data)
        # ONLY ALLOWED CONTENT TYPES
        allowed_content_types = ["question", "exam_reference", "normal_text", "header", "instruction"]
        
        for block in page_data.get("text_content", []):
            content_type = block.get("content_type", "")
            if content_type not in allowed_content_types:
                continue
                
            translated = block.get("translated_content", "") or ""
            original = block.get("content", "") or ""
            content = translated if translated.strip() else original
            if not content:
                continue
            x0 = block['position']['x0']
            font_size = block.get('font', {}).get('size', 12)
            y0 = _top_from_bottom(block['position']['y0'], font_size, page_h)
            try:
                lang = detect_language(self.data)
                fname = lang.capitalize()
                pdf.setFont(fname, font_size)
            except Exception:
                pdf.setFont("Helvetica", font_size)
            # draw white rect then text to hide original
            safe = clean(content)
            try:
                text_w = pdf.stringWidth(safe, pdf._fontname, font_size)
            except Exception:
                text_w = max(10, len(safe) * (font_size * 0.5))
            pdf.setFillColor(colors.white)
            pdf.rect(x0-2, y0-(font_size*0.25)-2, text_w+4, font_size+4, stroke=0, fill=1)
            pdf.setFillColor(colors.black)
            pdf.drawString(x0, y0, safe)

    def draw_layout_elements(self, pdf, page_data):
        page_h = _page_height_from(page_data)
        for el in page_data.get("layout_elements", []):
            if el.get("type") == "line":
                x0 = el['position']['x0']; y0b = el['position']['y0']
                x1 = el['position']['x1']; y1b = el['position']['y1']
                y0 = _top_from_bottom(y0b, 0, page_h); y1 = _top_from_bottom(y1b, 0, page_h)
                pdf.setStrokeColor(colors.black); pdf.setLineWidth(0.5)
                pdf.line(x0, y0, x1, y1)

    def draw_vector_symbols(self, pdf, page_data):
        page_h = _top_from_bottom(page_data)
        for s in page_data.get("vector_symbols", []):
            x0, y0o, x1, y1o = s['bbox']
            h = y1o - y0o
            y0 = _top_from_bottom(y0o, h, page_h)
            pdf.setStrokeColor(colors.black); pdf.setLineWidth(1)
            pdf.rect(x0, y0, x1 - x0, h, stroke=1, fill=0)

# -------------------- OVERLAY PDF GENERATOR (SAFE COVER) --------------------
class OverlayPDFGenerator:
    def __init__(self, json_data, original_pdf_path, output_pdf):
        if isinstance(json_data, str):
            with open(json_data, 'r', encoding='utf-8') as f:
                self.data = json.load(f)
        else:
            self.data = json_data
        self.original_pdf_path = original_pdf_path
        self.output_pdf = output_pdf
        self.font_name_map = {}
        self._register_fonts()

    def _register_fonts(self):
        try:
            for lang, font_path in FONTS.items():
                if os.path.exists(font_path):
                    name = f"Font_{lang}"
                    pdfmetrics.registerFont(TTFont(name, font_path))
                    self.font_name_map[lang] = name
        except Exception:
            pass

    def _clean(self, s):
        return clean(s)

    def _detect_language(self):
        return detect_language(self.data)

    def generate_pdf(self):
        if not os.path.exists(self.original_pdf_path):
            raise FileNotFoundError(self.original_pdf_path)
        reader = PdfReader(self.original_pdf_path)
        writer = PdfWriter()
        lang = self._detect_language()
        pages = self.data.get("pages", [])
        
        # ONLY ALLOWED CONTENT TYPES
        allowed_content_types = ["question", "exam_reference", "normal_text", "header", "instruction"]
        
        for i, page_data in enumerate(pages):
            if i >= len(reader.pages):
                continue
            base = reader.pages[i]
            dims = page_data.get("dimensions", {})
            w = float(dims.get("width", 595) or 595)
            h = float(dims.get("height", 842) or 842)
            packet = io.BytesIO()
            c = canvas.Canvas(packet, pagesize=(w, h))
            blocks_drawn = 0
            
            for block in page_data.get("text_content", []):
                # CHECK CONTENT TYPE - ONLY PROCESS ALLOWED TYPES
                content_type = block.get("content_type", "")
                if content_type not in allowed_content_types:
                    continue
                    
                translated = block.get("translated_content", "") or ""
                original = block.get("content", "") or ""
                content = translated if translated.strip() else original
                if not content:
                    continue
                pos = block.get("position", {})
                x0 = float(pos.get("x0", 0) or 0)
                y0_json = float(pos.get("y0", 0) or 0)
                font_size = float(block.get("font", {}).get("size", 12) or 12)
                # convert
                if y0_json > h * 0.9:
                    y0 = h - y0_json - font_size
                else:
                    y0 = y0_json
                if y0 < 0: y0 = 0
                if y0 > h - font_size: y0 = h - font_size
                use_translated = bool(translated.strip())
                if use_translated and lang in self.font_name_map:
                    fname = self.font_name_map[lang]
                    try:
                        c.setFont(fname, font_size)
                    except Exception:
                        c.setFont("Helvetica", font_size)
                        fname = "Helvetica"
                else:
                    fname = "Helvetica"
                    c.setFont(fname, font_size)
                safe = self._clean(content)
                lines = safe.splitlines() or [safe]
                lh = font_size * 1.15
                pad = max(2, font_size * 0.15)
                cur_y = y0
                for ln in lines:
                    try:
                        tw = c.stringWidth(ln or " ", fname, font_size)
                    except Exception:
                        tw = max(10, len(ln) * (font_size * 0.5))
                    descent = font_size * 0.2
                    rx = x0 - pad; ry = cur_y - descent - pad; rw = tw + pad * 2; rh = font_size + pad * 2
                    if rx < 0:
                        rw += rx; rx = 0
                    if ry < 0:
                        rh += ry; ry = 0
                    c.setFillColor(colors.white); c.rect(rx, ry, rw, rh, stroke=0, fill=1)
                    c.setFillColor(colors.black); c.drawString(x0, cur_y, ln or " ")
                    cur_y -= lh
                blocks_drawn += 1
            c.save(); packet.seek(0)
            try:
                overlay = PdfReader(packet).pages[0]
                try:
                    base.merge_page(overlay)
                except Exception:
                    try:
                        base.mergePage(overlay)
                    except Exception:
                        pass
                writer.add_page(base)
            except Exception:
                writer.add_page(base)
        with open(self.output_pdf, "wb") as out_f:
            writer.write(out_f)

# -------------------- CLI / Example --------------------
def pdf_creation_cli_main():
    JSON_PATH = r"E:\PDFExtraction\translated_jsons\sbi_extracted_hi.json"
    ORIGINAL_PDF = r"E:\PDFExtraction\SBI Clerk Prelims.pdf"
    OUTPUT_OVERLAY = "output_translated_overlay.pdf"
    # Use OverlayPDFGenerator (recommended)
    if not os.path.exists(JSON_PATH):
        print("JSON not found:", JSON_PATH)
    elif not os.path.exists(ORIGINAL_PDF):
        print("Original PDF not found:", ORIGINAL_PDF)
    else:
        gen = OverlayPDFGenerator(JSON_PATH, ORIGINAL_PDF, OUTPUT_OVERLAY)
        gen.generate_pdf()
        print("Done:", OUTPUT_OVERLAY)


# ======================================================================
# Unified Pipeline Orchestrator
# ======================================================================
def normalize_language_choices(selection: str | None) -> List[str]:
    """Convert comma-separated language input into ISO codes."""
    if not selection:
        return ["te"]
    normalized = []
    tokens = [token.strip() for token in selection.split(",") if token.strip()]
    for token in tokens:
        token_lower = token.lower()
        if token_lower in LANG_CODE_TO_NAME:
            normalized.append(token_lower)
        elif token_lower in LANG_NAME_TO_CODE:
            normalized.append(LANG_NAME_TO_CODE[token_lower])
        else:
            supported = ", ".join(f"{code} ({name})" for code, name in LANG_CODE_TO_NAME.items())
            raise ValueError(f"Unsupported language '{token}'. Supported options: {supported}")
    # Preserve ordering but drop duplicates
    seen = set()
    ordered = []
    for code in normalized:
        if code not in seen:
            ordered.append(code)
            seen.add(code)
    return ordered or ["te"]


def list_supported_languages():
    print("\nSupported Languages:")
    for code, name in LANG_CODE_TO_NAME.items():
        print(f"  - {code}: {name}")


def run_full_pipeline(pdf_path: str,
                      languages: List[str],
                      include_images: bool = True,
                      image_handling: str = "metadata",
                      extracted_json_path: str | None = None,
                      translated_dir: str = OUTPUT_DIR,
                      output_dir: str = "outputs",
                      overlay: bool = True):
    """Run extraction ‚Üí translation ‚Üí PDF creation workflow."""
    pdf_file = Path(pdf_path)
    if not pdf_file.exists():
        raise FileNotFoundError(f"PDF not found: {pdf_path}")
    
    output_dir_path = Path(output_dir)
    output_dir_path.mkdir(parents=True, exist_ok=True)
    
    translated_dir_path = Path(translated_dir)
    translated_dir_path.mkdir(parents=True, exist_ok=True)
    
    extracted_path = Path(extracted_json_path) if extracted_json_path else output_dir_path / f"{pdf_file.stem}_extracted.json"
    
    print("\n=== STEP 1: PDF ‚Üí JSON Extraction ===")
    converter = PDFToJSONConverter()
    converter.convert_pdf_to_json_enhanced(
        str(pdf_file),
        str(extracted_path),
        include_images=include_images,
        image_handling=image_handling,
    )
    change_mathematical_to_normal_text(str(extracted_path), str(extracted_path))
    update_content_types_to_mathematical(str(extracted_path), str(extracted_path))
    verify_json_structure(str(extracted_path))
    
    translated_results = []
    generated_pdfs = []
    
    print("\n=== STEP 2: JSON Translation ===")
    for lang_code in languages:
        lang_name = LANG_CODE_TO_NAME.get(lang_code, lang_code.upper())
        print(f"\n--- Translating into {lang_name} ({lang_code}) ---")
        translated_path = translate_json_file(
            str(extracted_path),
            lang_code,
            lang_name,
            output_dir=str(translated_dir_path)
        )
        if not translated_path:
            print(f"‚ö†Ô∏è  Translation skipped for {lang_name}")
            continue
        translated_results.append(translated_path)
        
        if overlay:
            print(f"--- Rebuilding PDF for {lang_name} ---")
            output_pdf = output_dir_path / f"{pdf_file.stem}_{lang_code}.pdf"
            try:
                generator = OverlayPDFGenerator(str(translated_path), str(pdf_file), str(output_pdf))
                generator.generate_pdf()
                generated_pdfs.append(output_pdf)
                print(f"‚úÖ Generated PDF: {output_pdf}")
            except Exception as pdf_error:
                print(f"‚ùå PDF generation failed for {lang_name}: {pdf_error}")
    
    print("\n=== PIPELINE SUMMARY ===")
    print(f"üìÑ Source PDF        : {pdf_file}")
    print(f"üßæ Extracted JSON    : {extracted_path}")
    print(f"üåê Languages         : {', '.join(languages)}")
    print(f"üíæ Translated JSONs  : {len(translated_results)} files in {translated_dir_path.resolve()}")
    if overlay:
        print(f"üìö Generated PDFs    : {len(generated_pdfs)} files in {output_dir_path.resolve()}")
    else:
        print("üìö Generated PDFs    : Skipped (overlay disabled)")
    
    return {
        "extracted_json": str(extracted_path),
        "translations": [str(path) for path in translated_results],
        "generated_pdfs": [str(path) for path in generated_pdfs],
    }


def build_arg_parser():
    parser = argparse.ArgumentParser(
        description="Unified PDF translation pipeline (extraction ‚Üí translation ‚Üí PDF rebuild)."
    )
    parser.add_argument("--pdf", required=True, help="Path to the source PDF file.")
    parser.add_argument("--languages", default="te",
                        help="Comma-separated ISO codes or language names (e.g., 'te,hi').")
    parser.add_argument("--no-images", action="store_true",
                        help="Skip image extraction to speed up processing.")
    parser.add_argument("--image-handling", choices=["metadata", "external", "base64"],
                        default="metadata", help="How to store extracted image data in JSON.")
    parser.add_argument("--extracted-json",
                        help="Optional path for the intermediate extracted JSON file.")
    parser.add_argument("--translated-dir", default="translated_jsons",
                        help="Directory to store translated JSON files.")
    parser.add_argument("--output-dir", default="outputs",
                        help="Directory to store extracted JSON and generated PDFs.")
    parser.add_argument("--no-overlay", action="store_true",
                        help="Skip PDF regeneration and produce only translated JSON.")
    parser.add_argument("--list-languages", action="store_true",
                        help="List supported languages and exit.")
    return parser


def main():
    parser = build_arg_parser()
    args = parser.parse_args()
    
    if args.list_languages:
        list_supported_languages()
        return
    
    try:
        languages = normalize_language_choices(args.languages)
    except ValueError as exc:
        parser.error(str(exc))
        return
    
    run_full_pipeline(
        pdf_path=args.pdf,
        languages=languages,
        include_images=not args.no_images,
        image_handling=args.image_handling,
        extracted_json_path=args.extracted_json,
        translated_dir=args.translated_dir,
        output_dir=args.output_dir,
        overlay=not args.no_overlay
    )


if __name__ == "__main__":
    main()
